{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # **Near Real-Time Probabilistic Plasma Flow Analysis using LSTM and DSCOVR Data from the Lagrangian Point (L1)** -->\n",
    "# **DSCOVRY - Advanced Data Preprocessing and Space Weather Forecasting using DSCOVR Data from the Lagrangian point (L1)**\n",
    "\n",
    "[DSCOVR](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ngdc.stp.swx:satellite-systems_dscovr) continues to operate beyond its expected lifespan, occasionally producing hardware faults that may result from space weather events. In this notebook, we analyze raw data from NASA's [DSCOVR](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ngdc.stp.swx:satellite-systems_dscovr) mission to forecast geomagnetic storms, taking into account hardware faults.\n",
    "\n",
    "For this project, we draw upon prior research, specifically:\n",
    "\n",
    "- [Cristoforetti, M., Battiston, R., Gobbi, A. et al. “Prominence of the training data preparation in geomagnetic storm prediction using deep neural networks.“](https://www.nature.com/articles/s41598-022-11721-8)  Sci Rep 12, 7631 (2022).\n",
    "- [Marina Stepanova et al. “Prediction of Geomagnetic Storm Using Neural Networks:Comparison of the Efficiency of the Satellite and GroundBased Input Parameters.“](https://iopscience.iop.org/article/10.1088/1742-6596/134/1/012041/pdf) 2008 J. Phys.: Conf. Ser. 134 012041\n",
    "- [Tasistro-Hart, Adrian et al. “Probabilistic Geomagnetic Storm Forecasting via Deep Learning.”](https://www.semanticscholar.org/paper/Probabilistic-Geomagnetic-Storm-Forecasting-via-Tasistro-Hart-Grayver/45103139959e86620de7353615db70e4e730efe5)  Journal of Geophysical Research: Space Physics 126 (2020): n. pag.\n",
    "- [Gulati, Ishita et al. “Classification based Detection of Geomagnetic Storms using LSTM Neural Network.”](https://www.semanticscholar.org/paper/Classification-based-Detection-of-Geomagnetic-using-Gulati-Li/96d8ff1d64f3d2691ee42e1b3ea6c33cc4136d6f) 2022 3rd URSI Atlantic and Asia Pacific Radio Science Meeting (AT-AP-RASC) (2022): 1-4.\n",
    "- [Smith, A. W., Forsyth, C., Rae, I. J., Garton, T. M., Jackman, C. M., Bakrania, M., et al. On the considerations of using near real time data for space weather hazard forecasting.](https://spiral.imperial.ac.uk/bitstream/10044/1/98383/2/Smith_SpaceWeather_20_e2022SW003098_2022.pdf) Space Weather, 20, (2022) e2022SW003098. https://doi.org/10.1029/2022SW003098\n",
    "\n",
    "Our objective is to implement the model in a real-time or near real-time environment, enabling us to process [DSCOVR](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ngdc.stp.swx:satellite-systems_dscovr) data and make predictions.\n",
    "\n",
    "## **Understanding Solar Wind**\n",
    "\n",
    "Solar wind frequently interacts with Earth's magnetosphere, leading to geomagnetic storms that can disrupt various technologies, including satellites and electrical power grids. \n",
    "To address this challenge, the National Oceanic and Atmospheric Administration (NOAA) operates a space weather station known as the Deep Space Climate Observatory (DSCOVR). \n",
    "\n",
    "[DSCOVR](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ngdc.stp.swx:satellite-systems_dscovr) is equipped with a range of sensors designed to predict these storms by collecting vital data on the speed, temperature, and density of incoming solar plasma.\n",
    "\n",
    "[DSCOVR](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ngdc.stp.swx:satellite-systems_dscovr) maintains a orbit, stationed at Lagrange point one, positioned 1.5 million kilometers from Earth and situated between our planet and the Sun. NOAA uses this data to simulate Earth's magnetic field and atmosphere, offering potential early warnings of geomagnetic storms.\n",
    "\n",
    "\n",
    "Geomagnetic activity is commonly quantified using the $D_{st}$ index. Using solar wind parameters and magnetic field data, we can potentially predict changes in the $D_{st}$ index. Prior studies recommend incorporating interplanetary parameters as inputs [[1]](https://www.nature.com/articles/s41598-022-11721-8), including the interplanetary magnetic field ($IMF$), solar wind ($SW$), and in some cases, specific components like the IMF $B_z$, $SW$ electric field, temperature, speed, and density.\n",
    "\n",
    "It is worth noting that the study by Tasistro-Hart et al. incorporates observations from the solar disk as additional input data, suggesting that this inclusion enhances forecasting reliability. Furthermore, they shift their focus from predicting $D_{st}$ to the external component of geomagnetic storms, known as $E_{st}$, and their neural network architecture is adept at forecasting uncertainty.\n",
    "\n",
    "Note that,\n",
    "\n",
    "$D_{st} = I_{st}(t) + E_{st}(t)$\n",
    "\n",
    "In this notebook, we build upon prior research in the field, specifically the work outlined in [[2]](https://www.semanticscholar.org/paper/Probabilistic-Geomagnetic-Storm-Forecasting-via-Tasistro-Hart-Grayver/45103139959e86620de7353615db70e4e730efe5). \n",
    "\n",
    "Our approach involves implementing a machine learning model while accounting for the impact of hardware faults and anomalies. Finally, our goal is to provide uncertainty estimates for the output, following the methodology described by Tasistro-Hart et al.\n",
    "\n",
    "However, it's important to note that meticulous data preparation for training and validation significantly influence the performance of the machine learning model.\n",
    "\n",
    "### **Data Resources**\n",
    "\n",
    "For this project, we will directly utilize raw [DSCOVR](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ngdc.stp.swx:satellite-systems_dscovr) data as input. Given our objective of building a near-real-time system, we have opted to integrate the most recent data by implementing a function that can download datasets from the [Experimental Data Repository](https://www.spaceappschallenge.org/develop-the-oracle-of-dscovr-experimental-data-repository/).\n",
    "\n",
    "These datasets encompass data starting from 2016 and continue to receive updates to the present day. They will be stored in the `dataset` folder. Each `.csv` file contains 53 columns, with column 0 representing the time in UTC in the following format:  `YYYY-MM-DD hh:mm:ss`. Columns 1-3 correspond to the magnetic field components measured in nanoteslas (nT) at the time indicated in column 0. The remaining columns contain dimensional measurements from the Faraday cup plasma detector.\n",
    "\n",
    "#### **References**\n",
    "[1] [Prominence of the training data preparation in geomagnetic storm prediction using deep neural networks](https://www.nature.com/articles/s41598-022-11721-8)\n",
    "\n",
    "[2] [Probabilistic Geomagnetic Storm Forecasting via Deep Learning](https://www.semanticscholar.org/paper/Probabilistic-Geomagnetic-Storm-Forecasting-via-Tasistro-Hart-Grayver/45103139959e86620de7353615db70e4e730efe5)\n",
    "- [DSCOVR: Deep Space Climate Observatory](https://www.nesdis.noaa.gov/current-satellite-missions/currently-flying/dscovr-deep-space-climate-observatory)\n",
    "- [DSCOVR (Deep Space Climate Observatory) -eoPortal](https://www.eoportal.org/satellite-missions/dscovr)\n",
    "- [Deep Space Climate Observatory (DSCOVR)](https://www.nist.gov/measuring-cosmos/deep-space-climate-observatory-dscovr)\n",
    "\n",
    "\n",
    "## **Data Retrieval and Preprocessing**\n",
    "\n",
    "For this prototype, we utilise the experimental data provided during the competition. It is worth noting that in a real-time production environment, the functionality of the following function can be extended to facilitate the real-time acquisition and storage of data in a MongoDB database for efficient retrieval and processing.\n",
    "\n",
    "A virtual environment was created and the required libraries were installed to develop this project. Navigate to the project folder and type the following:\n",
    "\n",
    "```\n",
    "python -m venv ./venv\n",
    "\n",
    "pip install -r requirements.txt\n",
    "\n",
    "``` \n",
    "\n",
    "To activate the virtual environment please type:\n",
    "\n",
    "```\n",
    "source ./venv/bin/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries for data retrival and pro-processing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mtp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas: 2.1.1\n",
      "Numpy: 1.26.0\n",
      "matplotlib: 3.8.0\n"
     ]
    }
   ],
   "source": [
    "# check versions\n",
    "\n",
    "print(\"Pandas: {}\".format(pd.__version__))\n",
    "print(\"Numpy: {}\".format(np.__version__))\n",
    "print(\"matplotlib: {}\".format(mtp.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "root_url = \"https://opensource.gsfc.nasa.gov/spaceappschallenge/\"\n",
    "# The data start being recorded from 2016; DSCOVR became operational on July 27, 2016.\n",
    "start_year = 2016\n",
    "today = dt.date.today()\n",
    "current_year = today.year\n",
    "\n",
    "dataset_folder = \"../dataset\"  # Dataset forlder to store .csv files/unzipped data\n",
    "tmp_folder = \"../tmp\"  # Temporary folder to store downloaded .zip files\n",
    "os.makedirs(tmp_folder, exist_ok=True)\n",
    "os.makedirs(dataset_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "def fetch_experimental_dscovr_data():\n",
    "    \"\"\"Download and store experimental DSCOVR data.\n",
    "\n",
    "    The data is stored in .zip folders, which are first downloaded to a tmp folder,\n",
    "    and then extracted into the dataset folder before the zip files are erased.\n",
    "\n",
    "    Note: If the file is from the current year, it is always downloaded, even if it already exists.\n",
    "    \"\"\"\n",
    "    for year in range(start_year, current_year + 1):\n",
    "        url = root_url + \"dsc_fc_summed_spectra_{}_v01.zip\".format(year)\n",
    "        zip_filename = os.path.join(tmp_folder, \"dscovr_data_{}.zip\".format(year))\n",
    "        dataset_year_folder = os.path.join(dataset_folder, str(year))\n",
    "\n",
    "        # Remove existing files for the current year (if they exist)\n",
    "        if year == current_year:\n",
    "            existing_files = [\n",
    "                f\n",
    "                for f in os.listdir(dataset_year_folder)\n",
    "                if f.startswith(f\"dsc_fc_summed_spectra_{year}\")\n",
    "            ]\n",
    "            for existing_file in existing_files:\n",
    "                os.remove(os.path.join(dataset_year_folder, existing_file))\n",
    "\n",
    "        # Check if the file already exists, if not, download it\n",
    "        if not any(\n",
    "            f.startswith(f\"dsc_fc_summed_spectra_{year}\")\n",
    "            for f in os.listdir(dataset_year_folder)\n",
    "        ):\n",
    "            try:\n",
    "                # Download the zip file\n",
    "                print(\"Downloading DSCOVR data for year {}...\".format(year))\n",
    "                response = requests.get(url, stream=True)\n",
    "                total_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    # Display a progress bar when downloading experimental DSCOVR dataset\n",
    "                    with open(zip_filename, \"wb\") as file, tqdm(\n",
    "                        desc=zip_filename,\n",
    "                        total=total_size,\n",
    "                        unit=\"B\",\n",
    "                        unit_scale=True,\n",
    "                        unit_divisor=1024,\n",
    "                    ) as bar:\n",
    "                        for data in response.iter_content(chunk_size=1024):\n",
    "                            file.write(data)\n",
    "                            bar.update(len(data))\n",
    "                    print(\"Download complete for year {}.\".format(year))\n",
    "\n",
    "                    # Unzip the downloaded file into the dataset folder\n",
    "                    with zipfile.ZipFile(zip_filename, \"r\") as zip_ref:\n",
    "                        zip_ref.extractall(dataset_year_folder)\n",
    "\n",
    "                    # Delete the downloaded zip file from the temporary folder\n",
    "                    os.remove(zip_filename)\n",
    "                else:\n",
    "                    print(\"Failed to download data for year {}.\".format(year))\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    \"An error occurred while downloading DSCOVR data for year {}: {}\".format(\n",
    "                        year, e\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            print(\"DSCOVR Data for year {} already exists.\".format(year))\n",
    "\n",
    "\n",
    "def prepare_experimental_Kp_data():\n",
    "    dataset_folder = '../dataset/'\n",
    "    kp_folder = 'kp_data'\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def prapare_dataframe():\n",
    "    \"\"\"Loop through the .csv files to create a Pandas DataFrame.\n",
    "\n",
    "    This function reads CSV files from the dataset folder for the specified range of years\n",
    "    and combines them into a single Pandas DataFrame. It handles missing files gracefully.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A combined DataFrame containing data from all available CSV files.\n",
    "    \"\"\"\n",
    "\n",
    "    combined_df = pd.DataFrame()\n",
    "    dataset_folder = '../dataset/'\n",
    "    for year in range(start_year, current_year + 1):\n",
    "        path_csv_file = os.path.join(dataset_folder, str(year), f\"dsc_fc_summed_spectra_{year}_v01.csv\")\n",
    "        # Check if the file exists before trying to read it\n",
    "        if os.path.exists(path_csv_file):\n",
    "            df = pd.read_csv(\n",
    "                path_csv_file,\n",
    "                delimiter=\",\",\n",
    "                parse_dates=[0],\n",
    "                na_values=\"0\",\n",
    "                header=None,\n",
    "            )\n",
    "            combined_df = pd.concat([combined_df, df])\n",
    "        else:\n",
    "            print(f\"Warning: File {path_csv_file} does not exist.\")\n",
    "            \n",
    "    return combined_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the functions to download data and create a dataframe\n",
    "\n",
    "fetch_experimental_dscovr_data()\n",
    "\n",
    "df = prapare_dataframe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploratory Data Analysis (EDA)**\n",
    "\n",
    "In this section, we employ Pandas and data visualization libraries to gain a deeper understanding of the experimental DSCOVR data. Our aim is to identify patterns that will inform the selection of the most suitable machine learning model for our problem statement.\n",
    "\n",
    "We begin by providing a high-level overview of the data. Subsequently, we proceed with data cleaning, addressing issues such as missing values. Next, we delve into the exploration of correlations between variables, presenting our findings through visual representations.  Finally, we summarise the key findings from this section.\n",
    "\n",
    "### **Data Summary**\n",
    "\n",
    "Following is a high level overview of the data. Additional information on how to interpret the columns can be found [here](https://hpde.io/NASA/NumericalData/OMNI/PT1H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>6.83609</td>\n",
       "      <td>-3.37934</td>\n",
       "      <td>-12.9205</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-01 00:01:00</td>\n",
       "      <td>6.76732</td>\n",
       "      <td>-3.30194</td>\n",
       "      <td>-12.9967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-01 00:02:00</td>\n",
       "      <td>6.39107</td>\n",
       "      <td>-2.61173</td>\n",
       "      <td>-13.3271</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-01 00:03:00</td>\n",
       "      <td>6.44897</td>\n",
       "      <td>-2.61525</td>\n",
       "      <td>-13.3299</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-01 00:04:00</td>\n",
       "      <td>6.58758</td>\n",
       "      <td>-2.73082</td>\n",
       "      <td>-13.2361</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0        1        2        3   4   5   6   7   8   9   ...  \\\n",
       "0 2016-01-01 00:00:00  6.83609 -3.37934 -12.9205 NaN NaN NaN NaN NaN NaN  ...   \n",
       "1 2016-01-01 00:01:00  6.76732 -3.30194 -12.9967 NaN NaN NaN NaN NaN NaN  ...   \n",
       "2 2016-01-01 00:02:00  6.39107 -2.61173 -13.3271 NaN NaN NaN NaN NaN NaN  ...   \n",
       "3 2016-01-01 00:03:00  6.44897 -2.61525 -13.3299 NaN NaN NaN NaN NaN NaN  ...   \n",
       "4 2016-01-01 00:04:00  6.58758 -2.73082 -13.2361 NaN NaN NaN NaN NaN NaN  ...   \n",
       "\n",
       "   44  45  46  47  48  49  50  51  52  53  \n",
       "0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "1 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "2 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "3 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "4 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "\n",
    "# A note regarding column 4-53: 5th+ columns: These are the \"raw\" measurements of the spectrum from the plasma detector so they are the \"level 1\" data. \n",
    "# We're reading it as each value is the flow strength of the solar wind, with each column being an interval in a range of flow speeds at which the solar wind is traveling at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3277440 entries, 0 to 175679\n",
      "Data columns (total 54 columns):\n",
      " #   Column  Dtype         \n",
      "---  ------  -----         \n",
      " 0   0       datetime64[ns]\n",
      " 1   1       float64       \n",
      " 2   2       float64       \n",
      " 3   3       float64       \n",
      " 4   4       float64       \n",
      " 5   5       float64       \n",
      " 6   6       float64       \n",
      " 7   7       float64       \n",
      " 8   8       float64       \n",
      " 9   9       float64       \n",
      " 10  10      float64       \n",
      " 11  11      float64       \n",
      " 12  12      float64       \n",
      " 13  13      float64       \n",
      " 14  14      float64       \n",
      " 15  15      float64       \n",
      " 16  16      float64       \n",
      " 17  17      float64       \n",
      " 18  18      float64       \n",
      " 19  19      float64       \n",
      " 20  20      float64       \n",
      " 21  21      float64       \n",
      " 22  22      float64       \n",
      " 23  23      float64       \n",
      " 24  24      float64       \n",
      " 25  25      float64       \n",
      " 26  26      float64       \n",
      " 27  27      float64       \n",
      " 28  28      float64       \n",
      " 29  29      float64       \n",
      " 30  30      float64       \n",
      " 31  31      float64       \n",
      " 32  32      float64       \n",
      " 33  33      float64       \n",
      " 34  34      float64       \n",
      " 35  35      float64       \n",
      " 36  36      float64       \n",
      " 37  37      float64       \n",
      " 38  38      float64       \n",
      " 39  39      float64       \n",
      " 40  40      float64       \n",
      " 41  41      float64       \n",
      " 42  42      float64       \n",
      " 43  43      float64       \n",
      " 44  44      float64       \n",
      " 45  45      float64       \n",
      " 46  46      float64       \n",
      " 47  47      float64       \n",
      " 48  48      float64       \n",
      " 49  49      float64       \n",
      " 50  50      float64       \n",
      " 51  51      float64       \n",
      " 52  52      float64       \n",
      " 53  53      float64       \n",
      "dtypes: datetime64[ns](1), float64(53)\n",
      "memory usage: 1.3 GB\n"
     ]
    }
   ],
   "source": [
    "# providing a concise summary of a dataframe.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3277440"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return the number of rows\n",
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3277440</td>\n",
       "      <td>3.259998e+06</td>\n",
       "      <td>3.259998e+06</td>\n",
       "      <td>3.259998e+06</td>\n",
       "      <td>1.519804e+06</td>\n",
       "      <td>1.582311e+06</td>\n",
       "      <td>1.721032e+06</td>\n",
       "      <td>1.809991e+06</td>\n",
       "      <td>1.922099e+06</td>\n",
       "      <td>2.008476e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>179435.000000</td>\n",
       "      <td>131650.000000</td>\n",
       "      <td>103457.000000</td>\n",
       "      <td>70114.000000</td>\n",
       "      <td>61407.000000</td>\n",
       "      <td>37701.000000</td>\n",
       "      <td>34288.000000</td>\n",
       "      <td>29597.000000</td>\n",
       "      <td>28458.000000</td>\n",
       "      <td>26270.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2019-11-24 16:42:18.717038592</td>\n",
       "      <td>8.015442e-02</td>\n",
       "      <td>-1.255092e-01</td>\n",
       "      <td>2.171055e-02</td>\n",
       "      <td>6.904954e+01</td>\n",
       "      <td>2.405151e+01</td>\n",
       "      <td>9.262740e+01</td>\n",
       "      <td>9.369958e+01</td>\n",
       "      <td>1.246801e+02</td>\n",
       "      <td>1.251655e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>410.258331</td>\n",
       "      <td>353.559684</td>\n",
       "      <td>397.477883</td>\n",
       "      <td>381.584273</td>\n",
       "      <td>351.028206</td>\n",
       "      <td>364.846009</td>\n",
       "      <td>364.814574</td>\n",
       "      <td>339.140779</td>\n",
       "      <td>387.293616</td>\n",
       "      <td>339.150626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>-2.014280e+01</td>\n",
       "      <td>-3.178510e+01</td>\n",
       "      <td>-3.332440e+01</td>\n",
       "      <td>2.317260e-01</td>\n",
       "      <td>2.317260e-01</td>\n",
       "      <td>2.317260e-01</td>\n",
       "      <td>2.317260e-01</td>\n",
       "      <td>2.317260e-01</td>\n",
       "      <td>2.317260e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>96.621000</td>\n",
       "      <td>63.826300</td>\n",
       "      <td>63.398300</td>\n",
       "      <td>2.675280</td>\n",
       "      <td>2.947210</td>\n",
       "      <td>59.301300</td>\n",
       "      <td>76.164100</td>\n",
       "      <td>65.362400</td>\n",
       "      <td>0.231726</td>\n",
       "      <td>2.469710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2018-01-04 11:59:45</td>\n",
       "      <td>-2.532720e+00</td>\n",
       "      <td>-2.640140e+00</td>\n",
       "      <td>-1.522248e+00</td>\n",
       "      <td>3.139748e+01</td>\n",
       "      <td>2.317260e-01</td>\n",
       "      <td>4.230860e+01</td>\n",
       "      <td>3.983035e+01</td>\n",
       "      <td>5.333525e+01</td>\n",
       "      <td>4.449867e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>392.940000</td>\n",
       "      <td>336.209750</td>\n",
       "      <td>387.257000</td>\n",
       "      <td>372.588500</td>\n",
       "      <td>339.152500</td>\n",
       "      <td>365.221000</td>\n",
       "      <td>370.332750</td>\n",
       "      <td>341.495000</td>\n",
       "      <td>397.712750</td>\n",
       "      <td>343.118500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2020-03-20 23:59:30</td>\n",
       "      <td>1.694915e-01</td>\n",
       "      <td>-2.131885e-01</td>\n",
       "      <td>3.084400e-02</td>\n",
       "      <td>5.893845e+01</td>\n",
       "      <td>1.063320e+01</td>\n",
       "      <td>8.518255e+01</td>\n",
       "      <td>8.183820e+01</td>\n",
       "      <td>1.085690e+02</td>\n",
       "      <td>1.006845e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>421.542000</td>\n",
       "      <td>367.358000</td>\n",
       "      <td>416.448000</td>\n",
       "      <td>409.860500</td>\n",
       "      <td>380.525000</td>\n",
       "      <td>397.663000</td>\n",
       "      <td>397.949500</td>\n",
       "      <td>373.665000</td>\n",
       "      <td>431.640000</td>\n",
       "      <td>381.959500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2021-10-10 23:59:15</td>\n",
       "      <td>2.672278e+00</td>\n",
       "      <td>2.395460e+00</td>\n",
       "      <td>1.553040e+00</td>\n",
       "      <td>9.426612e+01</td>\n",
       "      <td>3.352490e+01</td>\n",
       "      <td>1.191900e+02</td>\n",
       "      <td>1.199030e+02</td>\n",
       "      <td>1.635685e+02</td>\n",
       "      <td>1.546990e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>437.436000</td>\n",
       "      <td>385.025750</td>\n",
       "      <td>432.293000</td>\n",
       "      <td>423.639000</td>\n",
       "      <td>397.720000</td>\n",
       "      <td>418.133000</td>\n",
       "      <td>419.557750</td>\n",
       "      <td>394.069000</td>\n",
       "      <td>451.927750</td>\n",
       "      <td>401.169250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2023-05-02 23:59:00</td>\n",
       "      <td>3.304940e+01</td>\n",
       "      <td>2.789380e+01</td>\n",
       "      <td>3.483770e+01</td>\n",
       "      <td>1.675760e+03</td>\n",
       "      <td>1.582720e+03</td>\n",
       "      <td>1.736050e+03</td>\n",
       "      <td>1.496590e+03</td>\n",
       "      <td>1.699290e+03</td>\n",
       "      <td>1.848460e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>1756.870000</td>\n",
       "      <td>1757.440000</td>\n",
       "      <td>1775.960000</td>\n",
       "      <td>1762.550000</td>\n",
       "      <td>1689.330000</td>\n",
       "      <td>1719.110000</td>\n",
       "      <td>1939.020000</td>\n",
       "      <td>1852.740000</td>\n",
       "      <td>1875.050000</td>\n",
       "      <td>1866.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.410268e+00</td>\n",
       "      <td>3.765791e+00</td>\n",
       "      <td>2.982616e+00</td>\n",
       "      <td>6.505515e+01</td>\n",
       "      <td>4.948382e+01</td>\n",
       "      <td>7.784311e+01</td>\n",
       "      <td>8.532521e+01</td>\n",
       "      <td>1.040021e+02</td>\n",
       "      <td>1.253738e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>66.313573</td>\n",
       "      <td>66.450856</td>\n",
       "      <td>72.205229</td>\n",
       "      <td>84.113211</td>\n",
       "      <td>85.454188</td>\n",
       "      <td>99.350766</td>\n",
       "      <td>104.976669</td>\n",
       "      <td>109.746891</td>\n",
       "      <td>118.891957</td>\n",
       "      <td>112.803118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  0             1             2   \\\n",
       "count                        3277440  3.259998e+06  3.259998e+06   \n",
       "mean   2019-11-24 16:42:18.717038592  8.015442e-02 -1.255092e-01   \n",
       "min              2016-01-01 00:00:00 -2.014280e+01 -3.178510e+01   \n",
       "25%              2018-01-04 11:59:45 -2.532720e+00 -2.640140e+00   \n",
       "50%              2020-03-20 23:59:30  1.694915e-01 -2.131885e-01   \n",
       "75%              2021-10-10 23:59:15  2.672278e+00  2.395460e+00   \n",
       "max              2023-05-02 23:59:00  3.304940e+01  2.789380e+01   \n",
       "std                              NaN  3.410268e+00  3.765791e+00   \n",
       "\n",
       "                 3             4             5             6             7   \\\n",
       "count  3.259998e+06  1.519804e+06  1.582311e+06  1.721032e+06  1.809991e+06   \n",
       "mean   2.171055e-02  6.904954e+01  2.405151e+01  9.262740e+01  9.369958e+01   \n",
       "min   -3.332440e+01  2.317260e-01  2.317260e-01  2.317260e-01  2.317260e-01   \n",
       "25%   -1.522248e+00  3.139748e+01  2.317260e-01  4.230860e+01  3.983035e+01   \n",
       "50%    3.084400e-02  5.893845e+01  1.063320e+01  8.518255e+01  8.183820e+01   \n",
       "75%    1.553040e+00  9.426612e+01  3.352490e+01  1.191900e+02  1.199030e+02   \n",
       "max    3.483770e+01  1.675760e+03  1.582720e+03  1.736050e+03  1.496590e+03   \n",
       "std    2.982616e+00  6.505515e+01  4.948382e+01  7.784311e+01  8.532521e+01   \n",
       "\n",
       "                 8             9   ...             44             45  \\\n",
       "count  1.922099e+06  2.008476e+06  ...  179435.000000  131650.000000   \n",
       "mean   1.246801e+02  1.251655e+02  ...     410.258331     353.559684   \n",
       "min    2.317260e-01  2.317260e-01  ...      96.621000      63.826300   \n",
       "25%    5.333525e+01  4.449867e+01  ...     392.940000     336.209750   \n",
       "50%    1.085690e+02  1.006845e+02  ...     421.542000     367.358000   \n",
       "75%    1.635685e+02  1.546990e+02  ...     437.436000     385.025750   \n",
       "max    1.699290e+03  1.848460e+03  ...    1756.870000    1757.440000   \n",
       "std    1.040021e+02  1.253738e+02  ...      66.313573      66.450856   \n",
       "\n",
       "                  46            47            48            49            50  \\\n",
       "count  103457.000000  70114.000000  61407.000000  37701.000000  34288.000000   \n",
       "mean      397.477883    381.584273    351.028206    364.846009    364.814574   \n",
       "min        63.398300      2.675280      2.947210     59.301300     76.164100   \n",
       "25%       387.257000    372.588500    339.152500    365.221000    370.332750   \n",
       "50%       416.448000    409.860500    380.525000    397.663000    397.949500   \n",
       "75%       432.293000    423.639000    397.720000    418.133000    419.557750   \n",
       "max      1775.960000   1762.550000   1689.330000   1719.110000   1939.020000   \n",
       "std        72.205229     84.113211     85.454188     99.350766    104.976669   \n",
       "\n",
       "                 51            52            53  \n",
       "count  29597.000000  28458.000000  26270.000000  \n",
       "mean     339.140779    387.293616    339.150626  \n",
       "min       65.362400      0.231726      2.469710  \n",
       "25%      341.495000    397.712750    343.118500  \n",
       "50%      373.665000    431.640000    381.959500  \n",
       "75%      394.069000    451.927750    401.169250  \n",
       "max     1852.740000   1875.050000   1866.960000  \n",
       "std      109.746891    118.891957    112.803118  \n",
       "\n",
       "[8 rows x 54 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic descriptive statistics for all columns\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53],\n",
       "      dtype='int64')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return the list of column names of the dataframe\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Cleaning**\n",
    "\n",
    "In this section, our focus is on data cleaning, which involves addressing missing values and outliers early in the process. Missing values can adversely affect the accuracy of a machine learning model. There are a couple of options for handling them: you can either drop rows or columns with missing values, or you can fill in the missing values with the median, mean, or mode. To determine the best approach, you can count the number of `NaN` values in the dataframe and also understand the significance of these missing valies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           0\n",
       "1       17442\n",
       "2       17442\n",
       "3       17442\n",
       "4     1757636\n",
       "5     1695129\n",
       "6     1556408\n",
       "7     1467449\n",
       "8     1355341\n",
       "9     1268964\n",
       "10    1148804\n",
       "11    1053137\n",
       "12     921593\n",
       "13     853886\n",
       "14     732934\n",
       "15     682399\n",
       "16     623358\n",
       "17     554684\n",
       "18     506961\n",
       "19     472456\n",
       "20     425829\n",
       "21     425497\n",
       "22     434438\n",
       "23     521356\n",
       "24     573915\n",
       "25     749402\n",
       "26     846987\n",
       "27    1001870\n",
       "28    1124509\n",
       "29    1315618\n",
       "30    1457869\n",
       "31    1690322\n",
       "32    1784844\n",
       "33    2032300\n",
       "34    2114747\n",
       "35    2218250\n",
       "36    2396398\n",
       "37    2499624\n",
       "38    2570894\n",
       "39    2726544\n",
       "40    2775096\n",
       "41    2936463\n",
       "42    2979187\n",
       "43    3035621\n",
       "44    3098005\n",
       "45    3145790\n",
       "46    3173983\n",
       "47    3207326\n",
       "48    3216033\n",
       "49    3239739\n",
       "50    3243152\n",
       "51    3247843\n",
       "52    3248982\n",
       "53    3251170\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of missing values\n",
    "df.isnull().sum()\n",
    "# we have 3277440 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 0: 0 consecutive NaN values\n",
      "Column 1: 1679 consecutive NaN values\n",
      "Column 2: 1679 consecutive NaN values\n",
      "Column 3: 1679 consecutive NaN values\n",
      "Column 4: 34552 consecutive NaN values\n",
      "Column 5: 29218 consecutive NaN values\n",
      "Column 6: 28023 consecutive NaN values\n",
      "Column 7: 24729 consecutive NaN values\n",
      "Column 8: 24729 consecutive NaN values\n",
      "Column 9: 18278 consecutive NaN values\n",
      "Column 10: 16373 consecutive NaN values\n",
      "Column 11: 14466 consecutive NaN values\n",
      "Column 12: 14346 consecutive NaN values\n",
      "Column 13: 13511 consecutive NaN values\n",
      "Column 14: 13237 consecutive NaN values\n",
      "Column 15: 13022 consecutive NaN values\n",
      "Column 16: 9263 consecutive NaN values\n",
      "Column 17: 7859 consecutive NaN values\n",
      "Column 18: 7699 consecutive NaN values\n",
      "Column 19: 7508 consecutive NaN values\n",
      "Column 20: 6484 consecutive NaN values\n",
      "Column 21: 5123 consecutive NaN values\n",
      "Column 22: 4808 consecutive NaN values\n",
      "Column 23: 5494 consecutive NaN values\n",
      "Column 24: 9890 consecutive NaN values\n",
      "Column 25: 13214 consecutive NaN values\n",
      "Column 26: 13783 consecutive NaN values\n",
      "Column 27: 14786 consecutive NaN values\n",
      "Column 28: 15547 consecutive NaN values\n",
      "Column 29: 18141 consecutive NaN values\n",
      "Column 30: 18373 consecutive NaN values\n",
      "Column 31: 19430 consecutive NaN values\n",
      "Column 32: 34343 consecutive NaN values\n",
      "Column 33: 37268 consecutive NaN values\n",
      "Column 34: 37268 consecutive NaN values\n",
      "Column 35: 37320 consecutive NaN values\n",
      "Column 36: 37320 consecutive NaN values\n",
      "Column 37: 37417 consecutive NaN values\n",
      "Column 38: 37417 consecutive NaN values\n",
      "Column 39: 41696 consecutive NaN values\n",
      "Column 40: 50051 consecutive NaN values\n",
      "Column 41: 58596 consecutive NaN values\n",
      "Column 42: 60186 consecutive NaN values\n",
      "Column 43: 60614 consecutive NaN values\n",
      "Column 44: 107659 consecutive NaN values\n",
      "Column 45: 108021 consecutive NaN values\n",
      "Column 46: 108036 consecutive NaN values\n",
      "Column 47: 108036 consecutive NaN values\n",
      "Column 48: 108036 consecutive NaN values\n",
      "Column 49: 108036 consecutive NaN values\n",
      "Column 50: 108036 consecutive NaN values\n",
      "Column 51: 108036 consecutive NaN values\n",
      "Column 52: 108036 consecutive NaN values\n",
      "Column 53: 108036 consecutive NaN values\n"
     ]
    }
   ],
   "source": [
    "# Function to find the maximum consecutive NaN values\n",
    "def max_consecutive_nan(df):\n",
    "    max_consecutive = {}\n",
    "    for col in df.columns:\n",
    "        max_count = 0\n",
    "        current_count = 0\n",
    "        for value in df[col].isna():\n",
    "            if value:\n",
    "                current_count += 1\n",
    "                max_count = max(max_count, current_count)\n",
    "            else:\n",
    "                current_count = 0\n",
    "        max_consecutive[col] = max_count\n",
    "    return max_consecutive\n",
    "\n",
    "# Calculate maximum consecutive NaN values for each column\n",
    "max_consecutive = max_consecutive_nan(df)\n",
    "\n",
    "# Print the results\n",
    "for col, max_count in max_consecutive.items():\n",
    "    print(f\"Column {col}: {max_count} consecutive NaN values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a paper by Smith, A. W., Forsyth, C., Rae, I. J., Garton, T. M., Jackman, C. M., Bakrania, M., et al. (2022), it is noted that despite DSCOVR data being collected in real-time, there are instances where data from ACE are used to fill in missing data for larger intervals. Given the short duration of this project (2 days), we have chosen to employ limited interpolation to address missing values for gaps of 5 minutes or less, as suggested in the paper.\n",
    "\n",
    "Due to time constraints, we will use this method. There are different types of interpolation that can be applied to time series data, including:\n",
    "- linear interpolation (very common)\n",
    "- Polynomial interpolatin\n",
    "- Spline interpolation\n",
    "- Time-Based Interpolation\n",
    "- Exponential Smoothing\n",
    "- Seasonal Decomposition\n",
    "\n",
    "For this project we first tried linear interpolation for columns 4-53.  However it was clear that some columns had a very long sequences of consecutive NaN values. And more advanced techniques may be necessary.\n",
    "\n",
    "After linear interpolatio we use Time-series forecasting for the remanent missing values.\n",
    "```\n",
    "# count the number of missing values after linear interpolation\n",
    "interpolated_df.isnull().sum()\n",
    "-----------------------------------\n",
    "0           0\n",
    "1       17442\n",
    "2       17442\n",
    "3       17442\n",
    "4     1739252\n",
    "5     1671392\n",
    "6     1527983\n",
    "7     1433511\n",
    "8     1320848\n",
    "9     1230169\n",
    "10    1109591\n",
    "11    1011760\n",
    "12     881721\n",
    "13     810610\n",
    "14     696834\n",
    "15     644889\n",
    "16     587415\n",
    "17     520020\n",
    "18     473115\n",
    "19     437685\n",
    "20     390625\n",
    "21     383166\n",
    "22     390191\n",
    "23     467929\n",
    "24     515777\n",
    "...\n",
    "50    3225979\n",
    "51    3232589\n",
    "52    3234123\n",
    "53    3236703\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "A counter argument to employ an advanced method is that we might be able to use the presence of NaN for prediction. So what we decided to do is using different methods for differnt circumnstances. \n",
    "We drop columns that have consecutive NaN(s) values  for a period longer than 10 days. \n",
    "If the period is up to 10 days  we keep the rest and substitute it with a 0.0 type float64 after having used linear interpolation for gaps 5min or less, as this might be a fault related to a storm.\n",
    "\n",
    "The reason for this is that columns that have consecutive NaN values for such a long period (e.g., more than 10days or 2 weeks) are likely not relevant to geomagnetic storm analysis. It's reasonable to drop these columns from the dataset. We also decided to keep a record of the NaN missing data with their timestamp for later use. Note that geomagnetic storms usually last from several hours to several days. So I can use this information to decide which column to drop.\n",
    "\n",
    "To sum up we are allowing for different imputation methods based on the characteristics of the missing data and their potential relevance to geomagnetic storm analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_interpolation(df:  pd.DataFrame, start_column: int, end_column: int, gap: int):\n",
    "    \"\"\"\n",
    "    Linearly interpolate missing values in specified columns of a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the data to be interpolated.\n",
    "    init_column (int): The index of the first column to start interpolation.\n",
    "    end_column (int): The index of the last column to end interpolation (exclusive).\n",
    "    gap (int): The maximum gap size (in minutes) to be filled with interpolation.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with missing values interpolated.\n",
    "    \"\"\"\n",
    "    # Select the columns to be interpolated\n",
    "    columns_to_interpolate = df.columns[start_column:end_column]\n",
    "    # Apply linear interpolation to the specified columns\n",
    "    df[columns_to_interpolate] = df[columns_to_interpolate].interpolate(method='linear', limit=gap)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function and interpolate missing values with in 5min gap\n",
    "interpolated_df = linear_interpolation(df, start_column=4, end_column=54, gap=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dropping columns with long period of consecutive NaN values**\n",
    "\n",
    "We know there are 3277440 rows available, and we can calculate the thereshold for dropping columns as follow:\n",
    "\n",
    "- Calculate the total number of minutes in 14 consecutive days: 14 days x 24 hours/day x 60 minutes/hour = 20,160 minutes.\n",
    "\n",
    "- Calculate the percentage threshold: (20,160 minutes / 3,277,440 minutes) * 100% = 0.614%.\n",
    "\n",
    "based on our threshold criteria, we might consider dropping columns where there are consecutive NaN values for approximately 20,160 consecutive minutes. These long periods of missing data are less likely to be related to geomagnetic storm phenomena and may indicate other issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 0: 0 consecutive NaN values\n",
      "Column 1: 1679 consecutive NaN values\n",
      "Column 2: 1679 consecutive NaN values\n",
      "Column 3: 1679 consecutive NaN values\n",
      "Column 4: 34552 consecutive NaN values\n",
      "Column 5: 29218 consecutive NaN values\n",
      "Column 6: 28023 consecutive NaN values\n",
      "Column 7: 24729 consecutive NaN values\n",
      "Column 8: 24729 consecutive NaN values\n",
      "Column 9: 18278 consecutive NaN values\n",
      "Column 10: 16373 consecutive NaN values\n",
      "Column 11: 14466 consecutive NaN values\n",
      "Column 12: 14346 consecutive NaN values\n",
      "Column 13: 13511 consecutive NaN values\n",
      "Column 14: 13237 consecutive NaN values\n",
      "Column 15: 13022 consecutive NaN values\n",
      "Column 16: 9263 consecutive NaN values\n",
      "Column 17: 7859 consecutive NaN values\n",
      "Column 18: 7699 consecutive NaN values\n",
      "Column 19: 7508 consecutive NaN values\n",
      "Column 20: 6484 consecutive NaN values\n",
      "Column 21: 5123 consecutive NaN values\n",
      "Column 22: 4808 consecutive NaN values\n",
      "Column 23: 5494 consecutive NaN values\n",
      "Column 24: 9890 consecutive NaN values\n",
      "Column 25: 13214 consecutive NaN values\n",
      "Column 26: 13783 consecutive NaN values\n",
      "Column 27: 14786 consecutive NaN values\n",
      "Column 28: 15547 consecutive NaN values\n",
      "Column 29: 18141 consecutive NaN values\n",
      "Column 30: 18373 consecutive NaN values\n",
      "Column 31: 19430 consecutive NaN values\n",
      "Column 32: 34343 consecutive NaN values\n",
      "Column 33: 37268 consecutive NaN values\n",
      "Column 34: 37268 consecutive NaN values\n",
      "Column 35: 37320 consecutive NaN values\n",
      "Column 36: 37320 consecutive NaN values\n",
      "Column 37: 37417 consecutive NaN values\n",
      "Column 38: 37417 consecutive NaN values\n",
      "Column 39: 41696 consecutive NaN values\n",
      "Column 40: 50051 consecutive NaN values\n",
      "Column 41: 58596 consecutive NaN values\n",
      "Column 42: 60186 consecutive NaN values\n",
      "Column 43: 60614 consecutive NaN values\n",
      "Column 44: 107659 consecutive NaN values\n",
      "Column 45: 108021 consecutive NaN values\n",
      "Column 46: 108036 consecutive NaN values\n",
      "Column 47: 108036 consecutive NaN values\n",
      "Column 48: 108036 consecutive NaN values\n",
      "Column 49: 108036 consecutive NaN values\n",
      "Column 50: 108036 consecutive NaN values\n",
      "Column 51: 108036 consecutive NaN values\n",
      "Column 52: 108036 consecutive NaN values\n",
      "Column 53: 108036 consecutive NaN values\n",
      "Column 0: 0.00% max_consecutive_ratio\n",
      "Column 1: 0.05% max_consecutive_ratio\n",
      "Column 2: 0.05% max_consecutive_ratio\n",
      "Column 3: 0.05% max_consecutive_ratio\n",
      "Column 4: 1.05% max_consecutive_ratio\n",
      "Column 5: 0.89% max_consecutive_ratio\n",
      "Column 6: 0.86% max_consecutive_ratio\n",
      "Column 7: 0.75% max_consecutive_ratio\n",
      "Column 8: 0.75% max_consecutive_ratio\n",
      "Column 9: 0.56% max_consecutive_ratio\n",
      "Column 10: 0.50% max_consecutive_ratio\n",
      "Column 11: 0.44% max_consecutive_ratio\n",
      "Column 12: 0.44% max_consecutive_ratio\n",
      "Column 13: 0.41% max_consecutive_ratio\n",
      "Column 14: 0.40% max_consecutive_ratio\n",
      "Column 15: 0.40% max_consecutive_ratio\n",
      "Column 16: 0.28% max_consecutive_ratio\n",
      "Column 17: 0.24% max_consecutive_ratio\n",
      "Column 18: 0.23% max_consecutive_ratio\n",
      "Column 19: 0.23% max_consecutive_ratio\n",
      "Column 20: 0.20% max_consecutive_ratio\n",
      "Column 21: 0.16% max_consecutive_ratio\n",
      "Column 22: 0.15% max_consecutive_ratio\n",
      "Column 23: 0.17% max_consecutive_ratio\n",
      "Column 24: 0.30% max_consecutive_ratio\n",
      "Column 25: 0.40% max_consecutive_ratio\n",
      "Column 26: 0.42% max_consecutive_ratio\n",
      "Column 27: 0.45% max_consecutive_ratio\n",
      "Column 28: 0.47% max_consecutive_ratio\n",
      "Column 29: 0.55% max_consecutive_ratio\n",
      "Column 30: 0.56% max_consecutive_ratio\n",
      "Column 31: 0.59% max_consecutive_ratio\n",
      "Column 32: 1.05% max_consecutive_ratio\n",
      "Column 33: 1.14% max_consecutive_ratio\n",
      "Column 34: 1.14% max_consecutive_ratio\n",
      "Column 35: 1.14% max_consecutive_ratio\n",
      "Column 36: 1.14% max_consecutive_ratio\n",
      "Column 37: 1.14% max_consecutive_ratio\n",
      "Column 38: 1.14% max_consecutive_ratio\n",
      "Column 39: 1.27% max_consecutive_ratio\n",
      "Column 40: 1.53% max_consecutive_ratio\n",
      "Column 41: 1.79% max_consecutive_ratio\n",
      "Column 42: 1.84% max_consecutive_ratio\n",
      "Column 43: 1.85% max_consecutive_ratio\n",
      "Column 44: 3.28% max_consecutive_ratio\n",
      "Column 45: 3.30% max_consecutive_ratio\n",
      "Column 46: 3.30% max_consecutive_ratio\n",
      "Column 47: 3.30% max_consecutive_ratio\n",
      "Column 48: 3.30% max_consecutive_ratio\n",
      "Column 49: 3.30% max_consecutive_ratio\n",
      "Column 50: 3.30% max_consecutive_ratio\n",
      "Column 51: 3.30% max_consecutive_ratio\n",
      "Column 52: 3.30% max_consecutive_ratio\n",
      "Column 53: 3.30% max_consecutive_ratio\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate the maximum consecutive NaN values in a column\n",
    "\n",
    "def consecutive_nan_count(column):\n",
    "    max_consecutive = 0\n",
    "    current_consecutive = 0\n",
    "\n",
    "    for value in column:\n",
    "        if pd.isna(value):\n",
    "            current_consecutive += 1\n",
    "            max_consecutive = max(max_consecutive, current_consecutive)\n",
    "        else:\n",
    "            current_consecutive = 0  # Reset the counter for non-NaN values\n",
    "\n",
    "    return max_consecutive\n",
    "\n",
    "\n",
    "# Calculate the maximum consecutive NaN values for each column in the subset\n",
    "# Calculate and print the maximum consecutive NaN values for columns\n",
    "for col in interpolated_df.columns:  # Adjust the range to match columns \n",
    "    max_consecutive_nan = consecutive_nan_count(interpolated_df[col])\n",
    "    print(f\"Column {col}: {max_consecutive_nan} consecutive NaN values\")\n",
    "\n",
    "# Drop columns with more than 0.615% consecutive NaN values\n",
    "threshold = 0.00615 \n",
    "\n",
    "max_consecutive_values = {}\n",
    "column_index_map = {}  # Create a dictionary to map column index to name\n",
    "for column in interpolated_df.columns:  # Modify the range as needed\n",
    "    max_consecutive = consecutive_nan_count(interpolated_df[column])\n",
    "    max_consecutive_ratio = max_consecutive / len(interpolated_df.index)\n",
    "    max_consecutive_values[column] = max_consecutive_ratio\n",
    "# Drop columns that exceed the threshold\n",
    "for column, max_consecutive_ratio in max_consecutive_values.items():\n",
    "    print(f\"Column {column}: {max_consecutive_ratio:} max_consecutive_ratio\")\n",
    "    if max_consecutive_ratio > threshold:\n",
    "        interpolated_df.drop(column, axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the number of consecutive values for each columns:\n",
    "```Column 0: 0 consecutive NaN values\n",
    "Column 1: 1679 consecutive NaN values\n",
    "Column 2: 1679 consecutive NaN values\n",
    "Column 3: 1679 consecutive NaN values\n",
    "Column 4: 34517 consecutive NaN values\n",
    "Column 5: 29183 consecutive NaN values\n",
    "Column 6: 27988 consecutive NaN values\n",
    "Column 7: 24694 consecutive NaN values\n",
    "Column 8: 24694 consecutive NaN values\n",
    "Column 9: 18243 consecutive NaN values\n",
    "Column 10: 16338 consecutive NaN values\n",
    "Column 11: 14431 consecutive NaN values\n",
    "Column 12: 14311 consecutive NaN values\n",
    "Column 13: 13476 consecutive NaN values\n",
    "Column 14: 13202 consecutive NaN values\n",
    "Column 15: 12987 consecutive NaN values\n",
    "Column 16: 9228 consecutive NaN values\n",
    "Column 17: 7824 consecutive NaN values\n",
    "Column 18: 7664 consecutive NaN values\n",
    "Column 19: 7473 consecutive NaN values\n",
    "Column 20: 6449 consecutive NaN values\n",
    "Column 21: 5088 consecutive NaN values\n",
    "Column 22: 4773 consecutive NaN values\n",
    "Column 23: 5459 consecutive NaN values\n",
    "Column 24: 9855 consecutive NaN values\n",
    "Column 25: 13179 consecutive NaN values\n",
    "Column 26: 13748 consecutive NaN values\n",
    "Column 27: 14751 consecutive NaN values\n",
    "Column 28: 15512 consecutive NaN values\n",
    "Column 29: 18106 consecutive NaN values\n",
    "Column 30: 18338 consecutive NaN values\n",
    "Column 31: 19395 consecutive NaN values\n",
    "Column 32: 34308 consecutive NaN values\n",
    "Column 33: 37233 consecutive NaN values\n",
    "Column 34: 37233 consecutive NaN values\n",
    "Column 35: 37285 consecutive NaN values\n",
    "Column 36: 37285 consecutive NaN values\n",
    "Column 37: 37382 consecutive NaN values\n",
    "Column 38: 37382 consecutive NaN values\n",
    "Column 39: 41661 consecutive NaN values\n",
    "Column 40: 50016 consecutive NaN values\n",
    "Column 41: 58561 consecutive NaN values\n",
    "Column 42: 60151 consecutive NaN values\n",
    "Column 43: 60579 consecutive NaN values\n",
    "Column 44: 107624 consecutive NaN values\n",
    "Column 45: 107986 consecutive NaN values\n",
    "Column 46: 108001 consecutive NaN values\n",
    "Column 47: 108001 consecutive NaN values\n",
    "Column 48: 108001 consecutive NaN values\n",
    "Column 49: 108001 consecutive NaN values\n",
    "Column 50: 108001 consecutive NaN values\n",
    "Column 51: 108001 consecutive NaN values\n",
    "Column 52: 108001 consecutive NaN values\n",
    "Column 53: 108001 consecutive NaN values```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           0\n",
       "1       17442\n",
       "2       17442\n",
       "3       17442\n",
       "9     1230169\n",
       "10    1109591\n",
       "11    1011760\n",
       "12     881721\n",
       "13     810610\n",
       "14     696834\n",
       "15     644889\n",
       "16     587415\n",
       "17     520020\n",
       "18     473115\n",
       "19     437685\n",
       "20     390625\n",
       "21     383166\n",
       "22     390191\n",
       "23     467929\n",
       "24     515777\n",
       "25     686300\n",
       "26     778182\n",
       "27     932520\n",
       "28    1056520\n",
       "29    1244733\n",
       "30    1391951\n",
       "31    1622591\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the null values again\n",
    "interpolated_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Time-Series Forecasting to Predict Missing Data using Temporal Imputation Methods**\n",
    "\n",
    "Here we use time-series forecasting techniques like SARIMA or LSTM to predict the missing values based on the available data points. This approach is more sophisticated and can capture underlying patterns in the data. In this case we use SARIMA and LSTM. We will start with this method and complete the project, if time is left we will compare it to another interpolation/imputation method to see which one provide the best accuracy.\n",
    "\n",
    "Note you can choose from various optimization methods that are available in statsmodels. Some common choices include:\n",
    "\n",
    "- 'newton': Newton-Raphson method\n",
    "- 'bfgs': Broyden–Fletcher–Goldfarb–Shanno (BFGS) method (default)\n",
    "- 'lbfgs': Limited-memory BFGS\n",
    "- 'powell': Powell’s method\n",
    "- 'nm': Nelder-Mead method\n",
    "\n",
    "Again following is possible implementation we skipped in this project due to time constraint:\n",
    "```import statsmodels.api as sm\n",
    "\n",
    "# Define the SARIMA parameters (you may need to tune these)\n",
    "order = (1, 1, 1)  # ARIMA order (p, d, q)\n",
    "seasonal_order = (1, 1, 1, 12)  # Seasonal order (P, D, Q, S)\n",
    "\n",
    "# Forecast missing values for columns 4 and onward\n",
    "for col in interpolated_df.columns[4:]:\n",
    "    ts = interpolated_df[col]\n",
    "\n",
    "    # Split the data into observed (non-missing) and missing data\n",
    "    observed_data = ts.dropna()\n",
    "    missing_data = ts[ts.isnull()]\n",
    "\n",
    "    # Fit the SARIMA model\n",
    "    sarima_model = sm.tsa.statespace.SARIMAX(\n",
    "        observed_data,\n",
    "        order=order,\n",
    "        seasonal_order=seasonal_order,\n",
    "        enforce_stationarity=False,\n",
    "        enforce_invertibility=False,\n",
    "    )\n",
    "    sarima_results = sarima_model.fit(method='newton')\n",
    "\n",
    "    # Forecast missing values (replace NaNs with forecasts)\n",
    "    forecast_values = sarima_results.predict(\n",
    "        start=len(observed_data),\n",
    "        end=len(observed_data) + len(missing_data) - 1,  # Forecast for the entire length of the DataFrame\n",
    "        dynamic=False,\n",
    "    )\n",
    "    # Replace missing values with forecasted values\n",
    "    interpolated_df[col][ts.isnull()] = forecast_values\n",
    "\n",
    "\n",
    "# Plot the original and forecasted data for the first column\n",
    "mtp.figure(figsize=(12, 6))\n",
    "mtp.plot(interpolated_df[interpolated_df.columns[4]], label='Original Data',linestyle='-')\n",
    "mtp.plot(forecast_values, label='Forecasted Data', linestyle='--')\n",
    "mtp.xlabel('Date')\n",
    "mtp.ylabel('Value')\n",
    "mtp.title('SARIMA Forecasting for Column 4')\n",
    "mtp.show()\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Option3: LSTM to Deal with Missing Data**\n",
    "\n",
    "This methodology requires more computational resources and may be used for certain section of the dataset, however it is risky in case some information is loss.\n",
    "\n",
    "```\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def imputation_via_LSTM(df: pd.DataFrame, start_column: int):\n",
    "    \"\"\"\n",
    "    Impute missing values in the DataFrame using Long Short-Term Memory (LSTM) based on the specified columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing missing values.\n",
    "        start_column (int): The index of the first column to impute missing values from.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with missing values imputed using LSTM.\n",
    "    \"\"\"\n",
    "    # Extract the column names from the DataFrame\n",
    "    columns_to_impute = df.columns[start_column:]\n",
    "    # Define hyperparameters\n",
    "    n_steps = 10  # Number of time steps (sequence length)\n",
    "    n_features = len(columns_to_impute)  # Number of features (columns to impute)\n",
    "    n_epochs = 50\n",
    "    batch_size = 32\n",
    "\n",
    "    # Normalize the data (scaling)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(df[columns_to_impute])\n",
    "\n",
    "    # Create sequences\n",
    "    sequences = []\n",
    "    target_values = []\n",
    "    for i in range(len(df) - n_steps):\n",
    "        sequences.append(scaled_data[i:i+n_steps])\n",
    "        target_values.append(scaled_data[i+n_steps])\n",
    "\n",
    "    X = np.array(sequences)\n",
    "    y = np.array(target_values)\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    split_ratio = 0.8  # Adjust as needed\n",
    "    split_index = int(split_ratio * len(X))\n",
    "    X_train, X_test = X[:split_index], X[split_index:]\n",
    "    y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "    # Build an LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "    model.add(Dense(n_features))  # Output layer with the same number of features\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=n_epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "    # Predict missing values\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Inverse scaling to get the original values\n",
    "    y_pred = scaler.inverse_transform(y_pred)\n",
    "    y_test = scaler.inverse_transform(y_test)\n",
    "\n",
    "    # Calculate RMSE (Root Mean Squared Error) as an evaluation metric\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f'RMSE: {rmse}')\n",
    "\n",
    "    # Replace missing values in the original DataFrame with predicted values\n",
    "    df.loc[len(df) - len(y_test):, columns_to_impute] = y_pred\n",
    "\n",
    "    # Save the DataFrame with imputed values to a new file if needed\n",
    "    # df.to_csv('imputed_data.csv', index=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# call LTSM to deal with NaN data\n",
    "imputed_df = imputation_via_LSTM(df = interpolated_df, start_column=4)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have dropped the NaN values we do not need we need to update the one left with 0.0 float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>6.83609</td>\n",
       "      <td>-3.37934</td>\n",
       "      <td>-12.9205</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-01 00:01:00</td>\n",
       "      <td>6.76732</td>\n",
       "      <td>-3.30194</td>\n",
       "      <td>-12.9967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-01 00:02:00</td>\n",
       "      <td>6.39107</td>\n",
       "      <td>-2.61173</td>\n",
       "      <td>-13.3271</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-01 00:03:00</td>\n",
       "      <td>6.44897</td>\n",
       "      <td>-2.61525</td>\n",
       "      <td>-13.3299</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-01 00:04:00</td>\n",
       "      <td>6.58758</td>\n",
       "      <td>-2.73082</td>\n",
       "      <td>-13.2361</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0        1        2        3   9   10  11  12  13  14  ...  \\\n",
       "0 2016-01-01 00:00:00  6.83609 -3.37934 -12.9205 NaN NaN NaN NaN NaN NaN  ...   \n",
       "1 2016-01-01 00:01:00  6.76732 -3.30194 -12.9967 NaN NaN NaN NaN NaN NaN  ...   \n",
       "2 2016-01-01 00:02:00  6.39107 -2.61173 -13.3271 NaN NaN NaN NaN NaN NaN  ...   \n",
       "3 2016-01-01 00:03:00  6.44897 -2.61525 -13.3299 NaN NaN NaN NaN NaN NaN  ...   \n",
       "4 2016-01-01 00:04:00  6.58758 -2.73082 -13.2361 NaN NaN NaN NaN NaN NaN  ...   \n",
       "\n",
       "   22  23  24  25  26  27  28  29  30  31  \n",
       "0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "1 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "2 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "3 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "4 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3277440 entries, 0 to 175679\n",
      "Data columns (total 27 columns):\n",
      " #   Column  Dtype         \n",
      "---  ------  -----         \n",
      " 0   0       datetime64[ns]\n",
      " 1   1       float64       \n",
      " 2   2       float64       \n",
      " 3   3       float64       \n",
      " 4   9       float64       \n",
      " 5   10      float64       \n",
      " 6   11      float64       \n",
      " 7   12      float64       \n",
      " 8   13      float64       \n",
      " 9   14      float64       \n",
      " 10  15      float64       \n",
      " 11  16      float64       \n",
      " 12  17      float64       \n",
      " 13  18      float64       \n",
      " 14  19      float64       \n",
      " 15  20      float64       \n",
      " 16  21      float64       \n",
      " 17  22      float64       \n",
      " 18  23      float64       \n",
      " 19  24      float64       \n",
      " 20  25      float64       \n",
      " 21  26      float64       \n",
      " 22  27      float64       \n",
      " 23  28      float64       \n",
      " 24  29      float64       \n",
      " 25  30      float64       \n",
      " 26  31      float64       \n",
      "dtypes: datetime64[ns](1), float64(26)\n",
      "memory usage: 700.1 MB\n"
     ]
    }
   ],
   "source": [
    "interpolated_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Engineering** \n",
    "\n",
    "We also undertake feature engineering to create new attributes necessary for training our machine learning model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
