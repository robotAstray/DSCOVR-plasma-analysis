{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Near Real-time Plasma Flow Analysis using DSCOVR Data**\n",
    "\n",
    "DSCOVR continues to operate beyond its expected lifespan, occasionally producing hardware faults that may result from space weather events. In this notebook, we analyze raw data from NASA's DSCOVR mission to forecast geomagnetic storms, taking into account hardware faults.\n",
    "\n",
    "For this project, we draw upon prior research, specifically:\n",
    "\n",
    "- *i.* [Prominence of the training data preparation in geomagnetic storm prediction using deep neural networks](https://www.nature.com/articles/s41598-022-11721-8)\n",
    "- *ii.* [Prediction of Geomagnetic Storm Using Neural Networks:Comparison of the Efficiency of the Satellite and GroundBased Input Parameters](https://iopscience.iop.org/article/10.1088/1742-6596/134/1/012041/pdf)\n",
    "\n",
    "Our objective is to implement the model in a real-time or near real-time environment, enabling us to process DSCOVR data and make predictions.\n",
    "\n",
    "## **Understanding Solar Wind**\n",
    "\n",
    "Solar wind frequently interacts with Earth's magnetosphere, leading to geomagnetic storms that can disrupt various technologies, including satellites and electrical power grids. Consequently, the National Oceanic and Atmospheric Administration (NOAA) operates a space weather station known as the Deep Space Climate Observatory (DSCOVR). DSCOVR employs various sensors to facilitate the prediction of these storms by gathering data on the speed, temperature, and density of incoming solar plasma.\n",
    "\n",
    "DSCOVR orbits at a unique location, Lagrange point one, situated 1.5 million kilometers from Earth between the Earth and the Sun. This strategic position allows it to record data on incoming solar plasma before it reaches Earth's vicinity. NOAA leverages this data to simulate the state of Earth's magnetic field and atmosphere, potentially providing early warnings of geomagnetic storms.\n",
    "\n",
    "In general, the $D_{st}$ index is used to measure geomagnetic activity. By utilizing solar wind parameters and magnetic field data, we may be able to predict the $D_{st}$ index. Previous studies suggest using interplanetary parameters as inputs [[1]](https://www.nature.com/articles/s41598-022-11721-8), such as the interplanetary magnetic field ($IMF$), solar wind ($SW$), and in some studies, the IMF $B_z$ component, $SW$ electric field, temperature, speed, and density.\n",
    "\n",
    "In any event, the most crucial aspect is the data preparation for training and validation, as it plays a significant role in ensuring optimal ML model performance.\n",
    "\n",
    "### **References**\n",
    "[1] [Prominence of the training data preparation in geomagnetic storm prediction using deep neural networks](https://www.nature.com/articles/s41598-022-11721-8)\n",
    "- [DSCOVR: Deep Space Climate Observatory](https://www.nesdis.noaa.gov/current-satellite-missions/currently-flying/dscovr-deep-space-climate-observatory)\n",
    "- [DSCOVR (Deep Space Climate Observatory) -eoPortal](https://www.eoportal.org/satellite-missions/dscovr)\n",
    "- [Deep Space Climate Observatory (DSCOVR)](https://www.nist.gov/measuring-cosmos/deep-space-climate-observatory-dscovr)\n",
    "\n",
    "## **Data Resources**\n",
    "\n",
    "For this project, we will directly utilize raw DSCOVR data as input. Given our objective of building a near-real-time system, we have opted to integrate the most recent data by implementing a function that can download datasets from the [Experimental Data Repository](https://www.spaceappschallenge.org/develop-the-oracle-of-dscovr-experimental-data-repository/).\n",
    "\n",
    "These datasets encompass data starting from 2016 and continue to receive updates to the present day. They will be stored in the `dataset` folder. Each `.csv` file contains 53 columns, with column 0 representing the time in UTC in the following format:  `YYYY-MM-DD hh:mm:ss`. Columns 1-3 correspond to the magnetic field components measured in nanoteslas (nT) at the time indicated in column 0. The remaining columns contain dimensional measurements from the Faraday cup plasma detector.\n",
    "\n",
    "## **Data Retrieval and Preprocessing**\n",
    "\n",
    "For this prototype, we utilise the experimental data provided during the competition. It is worth noting that in a real-time production environment, the functionality of the following function can be extended to facilitate the real-time acquisition and storage of data in a MongoDB database for efficient retrieval and processing.\n",
    "\n",
    "A virtual environment was created and the required libraries were installed to develop this project. Navigate to the project folder and type the following:\n",
    "\n",
    "```\n",
    "python -m venv ./venv\n",
    "\n",
    "pip install -r requirements.txt\n",
    "\n",
    "``` \n",
    "\n",
    "To activate the virtual environment please type:\n",
    "\n",
    "```\n",
    "source ./venv/bin/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries for data retrival and pro-processing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mtp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas: 2.1.1\n",
      "Numpy: 1.26.0\n",
      "matplotlib: 3.8.0\n"
     ]
    }
   ],
   "source": [
    "# check versions \n",
    "\n",
    "print('Pandas: {}'.format(pd.__version__))\n",
    "print('Numpy: {}'.format(np.__version__))\n",
    "print('matplotlib: {}'.format(mtp.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for year 2016...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../dataset/dscovr_data_2016.zip: 100%|██████████| 20.5M/20.5M [00:08<00:00, 2.47MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete for year 2016.\n",
      "Downloading data for year 2017...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../dataset/dscovr_data_2017.zip: 100%|██████████| 38.4M/38.4M [00:12<00:00, 3.31MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete for year 2017.\n",
      "Downloading data for year 2018...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../dataset/dscovr_data_2018.zip: 100%|██████████| 38.3M/38.3M [00:12<00:00, 3.20MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete for year 2018.\n",
      "Downloading data for year 2019...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../dataset/dscovr_data_2019.zip: 100%|██████████| 18.2M/18.2M [00:05<00:00, 3.66MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete for year 2019.\n",
      "Downloading data for year 2020...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../dataset/dscovr_data_2020.zip: 100%|██████████| 35.2M/35.2M [01:12<00:00, 506kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete for year 2020.\n",
      "Downloading data for year 2021...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../dataset/dscovr_data_2021.zip: 100%|██████████| 49.8M/49.8M [00:19<00:00, 2.61MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete for year 2021.\n",
      "Downloading data for year 2022...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../dataset/dscovr_data_2022.zip: 100%|██████████| 55.2M/55.2M [00:18<00:00, 3.17MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete for year 2022.\n",
      "Downloading data for year 2023...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../dataset/dscovr_data_2023.zip: 100%|██████████| 17.2M/17.2M [00:04<00:00, 4.34MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete for year 2023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm \n",
    "\n",
    "root_url = 'https://opensource.gsfc.nasa.gov/spaceappschallenge/'\n",
    "start_year = 2016 # The data start from 2016 as the project\n",
    "today = dt.date.today()\n",
    "current_year = today.year\n",
    "\n",
    "dataset_folder = '../dataset'\n",
    "os.makedirs(dataset_folder, exist_ok=True)\n",
    "\n",
    "def fetch_experimental_dscovr_data():\n",
    "    \"\"\"Function to download and store experimental data in the dataset folder as .csv files\"\"\"\n",
    "    for year in range(start_year, current_year+1, 1):\n",
    "        url = root_url + \"dsc_fc_summed_spectra_{}_v01.zip\".format(year)\n",
    "        filename = os.path.join(dataset_folder, \"dscovr_data_{}.zip\".format(year))\n",
    "\n",
    "        # Check if the file already exists, if not, download it\n",
    "        if not os.path.exists(filename):\n",
    "            print(\"Downloading data for year {}...\".format(year))\n",
    "            response = requests.get(url, stream=True)\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            # Display a progress bar when downloading experimental DSCOVR dataset\n",
    "            with open(filename, 'wb') as file, tqdm(\n",
    "                    desc=filename,\n",
    "                    total=total_size,\n",
    "                    unit='B',\n",
    "                    unit_scale=True,\n",
    "                    unit_divisor=1024,\n",
    "            ) as bar:\n",
    "                for data in response.iter_content(chunk_size=1024):\n",
    "                    file.write(data)\n",
    "                    bar.update(len(data))\n",
    "            print(\"Download complete for year {}.\".format(year))\n",
    "        else:\n",
    "            print(\"Data for year {} already exists.\".format(year))\n",
    "\n",
    "fetch_experimental_dscovr_data()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
