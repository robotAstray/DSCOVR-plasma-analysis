{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Near Real-time Plasma Flow Analysis using DSCOVR Data**\n",
    "\n",
    "DSCOVR continues to operate beyond its expected lifespan, occasionally producing hardware faults that may result from space weather events. In this notebook, we analyze raw data from NASA's DSCOVR mission to forecast geomagnetic storms, taking into account hardware faults.\n",
    "\n",
    "For this project, we draw upon prior research, specifically:\n",
    "\n",
    "- *i.* [Prominence of the training data preparation in geomagnetic storm prediction using deep neural networks](https://www.nature.com/articles/s41598-022-11721-8)\n",
    "- *ii.* [Prediction of Geomagnetic Storm Using Neural Networks:Comparison of the Efficiency of the Satellite and GroundBased Input Parameters](https://iopscience.iop.org/article/10.1088/1742-6596/134/1/012041/pdf)\n",
    "\n",
    "Our objective is to implement the model in a real-time or near real-time environment, enabling us to process DSCOVR data and make predictions.\n",
    "\n",
    "## **Understanding Solar Wind**\n",
    "\n",
    "Solar wind frequently interacts with Earth's magnetosphere, leading to geomagnetic storms that can disrupt various technologies, including satellites and electrical power grids. Consequently, the National Oceanic and Atmospheric Administration (NOAA) operates a space weather station known as the Deep Space Climate Observatory (DSCOVR). DSCOVR employs various sensors to facilitate the prediction of these storms by gathering data on the speed, temperature, and density of incoming solar plasma.\n",
    "\n",
    "DSCOVR orbits at a unique location, Lagrange point one, situated 1.5 million kilometers from Earth between the Earth and the Sun. This strategic position allows it to record data on incoming solar plasma before it reaches Earth's vicinity. NOAA leverages this data to simulate the state of Earth's magnetic field and atmosphere, potentially providing early warnings of geomagnetic storms.\n",
    "\n",
    "In general, the $D_{st}$ index is used to measure geomagnetic activity. By utilizing solar wind parameters and magnetic field data, we may be able to predict the $D_{st}$ index. Previous studies suggest using interplanetary parameters as inputs [[1]](https://www.nature.com/articles/s41598-022-11721-8), such as the interplanetary magnetic field ($IMF$), solar wind ($SW$), and in some studies, the IMF $B_z$ component, $SW$ electric field, temperature, speed, and density.\n",
    "\n",
    "In any event, the most crucial aspect is the data preparation for training and validation, as it plays a significant role in ensuring optimal ML model performance.\n",
    "\n",
    "### **References**\n",
    "[1] [Prominence of the training data preparation in geomagnetic storm prediction using deep neural networks](https://www.nature.com/articles/s41598-022-11721-8)\n",
    "- [DSCOVR: Deep Space Climate Observatory](https://www.nesdis.noaa.gov/current-satellite-missions/currently-flying/dscovr-deep-space-climate-observatory)\n",
    "- [DSCOVR (Deep Space Climate Observatory) -eoPortal](https://www.eoportal.org/satellite-missions/dscovr)\n",
    "- [Deep Space Climate Observatory (DSCOVR)](https://www.nist.gov/measuring-cosmos/deep-space-climate-observatory-dscovr)\n",
    "\n",
    "## **Data Resources**\n",
    "\n",
    "For this project, we will directly utilize raw DSCOVR data as input. Given our objective of building a near-real-time system, we have opted to integrate the most recent data by implementing a function that can download datasets from the [Experimental Data Repository](https://www.spaceappschallenge.org/develop-the-oracle-of-dscovr-experimental-data-repository/).\n",
    "\n",
    "These datasets encompass data starting from 2016 and continue to receive updates to the present day. They will be stored in the `dataset` folder. Each `.csv` file contains 53 columns, with column 0 representing the time in UTC in the following format:  `YYYY-MM-DD hh:mm:ss`. Columns 1-3 correspond to the magnetic field components measured in nanoteslas (nT) at the time indicated in column 0. The remaining columns contain dimensional measurements from the Faraday cup plasma detector.\n",
    "\n",
    "## **Data Retrieval and Preprocessing**\n",
    "\n",
    "For this prototype, we utilise the experimental data provided during the competition. It is worth noting that in a real-time production environment, the functionality of the following function can be extended to facilitate the real-time acquisition and storage of data in a MongoDB database for efficient retrieval and processing.\n",
    "\n",
    "A virtual environment was created and the required libraries were installed to develop this project. Navigate to the project folder and type the following:\n",
    "\n",
    "```\n",
    "python -m venv ./venv\n",
    "\n",
    "pip install -r requirements.txt\n",
    "\n",
    "``` \n",
    "\n",
    "To activate the virtual environment please type:\n",
    "\n",
    "```\n",
    "source ./venv/bin/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries for data retrival and pro-processing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mtp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas: 2.1.1\n",
      "Numpy: 1.26.0\n",
      "matplotlib: 3.8.0\n"
     ]
    }
   ],
   "source": [
    "# check versions\n",
    "\n",
    "print(\"Pandas: {}\".format(pd.__version__))\n",
    "print(\"Numpy: {}\".format(np.__version__))\n",
    "print(\"matplotlib: {}\".format(mtp.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "root_url = \"https://opensource.gsfc.nasa.gov/spaceappschallenge/\"\n",
    "# The data start being recorded from 2016\n",
    "start_year = 2016\n",
    "today = dt.date.today()\n",
    "current_year = today.year\n",
    "\n",
    "dataset_folder = \"../dataset\"  # Dataset forlder to store .csv files/unzipped data\n",
    "tmp_folder = \"../tmp\"  # Temporary folder to store downloaded .zip files\n",
    "os.makedirs(tmp_folder, exist_ok=True)\n",
    "os.makedirs(dataset_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "def fetch_experimental_dscovr_data():\n",
    "    \"\"\"Download and store experimental DSCOVR data.\n",
    "\n",
    "    The data is stored in .zip folders, which are first downloaded to a tmp folder,\n",
    "    and then extracted into the dataset folder before the zip files are erased.\n",
    "\n",
    "    Note: If the file is from the current year, it is always downloaded, even if it already exists.\n",
    "    \"\"\"\n",
    "    for year in range(start_year, current_year + 1):\n",
    "        url = root_url + \"dsc_fc_summed_spectra_{}_v01.zip\".format(year)\n",
    "        zip_filename = os.path.join(tmp_folder, \"dscovr_data_{}.zip\".format(year))\n",
    "        dataset_year_folder = os.path.join(dataset_folder, str(year))\n",
    "\n",
    "        # Remove existing files for the current year (if they exist)\n",
    "        if year == current_year:\n",
    "            existing_files = [\n",
    "                f\n",
    "                for f in os.listdir(dataset_year_folder)\n",
    "                if f.startswith(f\"dsc_fc_summed_spectra_{year}\")\n",
    "            ]\n",
    "            for existing_file in existing_files:\n",
    "                os.remove(os.path.join(dataset_year_folder, existing_file))\n",
    "\n",
    "        # Check if the file already exists, if not, download it\n",
    "        if not any(\n",
    "            f.startswith(f\"dsc_fc_summed_spectra_{year}\")\n",
    "            for f in os.listdir(dataset_year_folder)\n",
    "        ):\n",
    "            try:\n",
    "                # Download the zip file\n",
    "                print(\"Downloading DSCOVR data for year {}...\".format(year))\n",
    "                response = requests.get(url, stream=True)\n",
    "                total_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    # Display a progress bar when downloading experimental DSCOVR dataset\n",
    "                    with open(zip_filename, \"wb\") as file, tqdm(\n",
    "                        desc=zip_filename,\n",
    "                        total=total_size,\n",
    "                        unit=\"B\",\n",
    "                        unit_scale=True,\n",
    "                        unit_divisor=1024,\n",
    "                    ) as bar:\n",
    "                        for data in response.iter_content(chunk_size=1024):\n",
    "                            file.write(data)\n",
    "                            bar.update(len(data))\n",
    "                    print(\"Download complete for year {}.\".format(year))\n",
    "\n",
    "                    # Unzip the downloaded file into the dataset folder\n",
    "                    with zipfile.ZipFile(zip_filename, \"r\") as zip_ref:\n",
    "                        zip_ref.extractall(dataset_year_folder)\n",
    "\n",
    "                    # Delete the downloaded zip file from the temporary folder\n",
    "                    os.remove(zip_filename)\n",
    "                else:\n",
    "                    print(\"Failed to download data for year {}.\".format(year))\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    \"An error occurred while downloading DSCOVR data for year {}: {}\".format(\n",
    "                        year, e\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            print(\"DSCOVR Data for year {} already exists.\".format(year))\n",
    "\n",
    "\n",
    "# fetch_experimental_dscovr_data()\n",
    "\n",
    "\n",
    "def prapare_dataframe():\n",
    "    \"\"\"Loop through the .csv files to create a Pandas DataFrame.\n",
    "\n",
    "    This function reads CSV files from the dataset folder for the specified range of years\n",
    "    and combines them into a single Pandas DataFrame. It handles missing files gracefully.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A combined DataFrame containing data from all available CSV files.\n",
    "    \"\"\"\n",
    "\n",
    "    combined_df = pd.DataFrame()\n",
    "    dataset_folder = '../dataset/'\n",
    "    for year in range(start_year, current_year + 1):\n",
    "        path_csv_file = os.path.join(dataset_folder, str(year), f\"dsc_fc_summed_spectra_{year}_v01.csv\")\n",
    "        # Check if the file exists before trying to read it\n",
    "        if os.path.exists(path_csv_file):\n",
    "            df = pd.read_csv(\n",
    "                path_csv_file,\n",
    "                delimiter=\",\",\n",
    "                parse_dates=[0],\n",
    "                na_values=\"0\",\n",
    "                header=None,\n",
    "            )\n",
    "            combined_df = pd.concat([combined_df, df])\n",
    "        else:\n",
    "            print(f\"Warning: File {path_csv_file} does not exist.\")\n",
    "            \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the functions to download data and create a dataframe\n",
    "\n",
    "fetch_experimental_dscovr_data()\n",
    "\n",
    "df = prapare_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
