{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # **Near Real-Time Probabilistic Plasma Flow Analysis using LSTM and DSCOVR Data from the Lagrangian Point (L1)** -->\n",
    "# **Near Real-Time Probabilistic Plasma Flow Analysis using LSTM, Dense Layers and DSCOVR Data from the Lagrangian point (L1)**\n",
    "\n",
    "[DSCOVR](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ngdc.stp.swx:satellite-systems_dscovr) continues to operate beyond its expected lifespan, occasionally producing hardware faults that may result from space weather events. In this notebook, we analyze raw data from NASA's [DSCOVR](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ngdc.stp.swx:satellite-systems_dscovr) mission to forecast geomagnetic storms, taking into account hardware faults.\n",
    "\n",
    "For this project, we draw upon prior research, specifically:\n",
    "\n",
    "- [Cristoforetti, M., Battiston, R., Gobbi, A. et al. “Prominence of the training data preparation in geomagnetic storm prediction using deep neural networks.“](https://www.nature.com/articles/s41598-022-11721-8)  Sci Rep 12, 7631 (2022).\n",
    "- [Marina Stepanova et al. “Prediction of Geomagnetic Storm Using Neural Networks:Comparison of the Efficiency of the Satellite and GroundBased Input Parameters.“](https://iopscience.iop.org/article/10.1088/1742-6596/134/1/012041/pdf) 2008 J. Phys.: Conf. Ser. 134 012041\n",
    "- [Tasistro-Hart, Adrian et al. “Probabilistic Geomagnetic Storm Forecasting via Deep Learning.”](https://www.semanticscholar.org/paper/Probabilistic-Geomagnetic-Storm-Forecasting-via-Tasistro-Hart-Grayver/45103139959e86620de7353615db70e4e730efe5)  Journal of Geophysical Research: Space Physics 126 (2020): n. pag.\n",
    "- [Gulati, Ishita et al. “Classification based Detection of Geomagnetic Storms using LSTM Neural Network.”](https://www.semanticscholar.org/paper/Classification-based-Detection-of-Geomagnetic-using-Gulati-Li/96d8ff1d64f3d2691ee42e1b3ea6c33cc4136d6f) 2022 3rd URSI Atlantic and Asia Pacific Radio Science Meeting (AT-AP-RASC) (2022): 1-4.\n",
    "- [Smith, A. W., Forsyth, C., Rae, I. J., Garton, T. M., Jackman, C. M., Bakrania, M., et al. On the considerations of using near real time data for space\n",
    "weather hazard forecasting.](https://spiral.imperial.ac.uk/bitstream/10044/1/98383/2/Smith_SpaceWeather_20_e2022SW003098_2022.pdf) Space Weather, 20, (2022) e2022SW003098. https://doi.org/10.1029/2022SW003098\n",
    "\n",
    "Our objective is to implement the model in a real-time or near real-time environment, enabling us to process [DSCOVR](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ngdc.stp.swx:satellite-systems_dscovr) data and make predictions.\n",
    "\n",
    "## **Understanding Solar Wind**\n",
    "\n",
    "Solar wind frequently interacts with Earth's magnetosphere, leading to geomagnetic storms that can disrupt various technologies, including satellites and electrical power grids. \n",
    "To address this challenge, the National Oceanic and Atmospheric Administration (NOAA) operates a space weather station known as the Deep Space Climate Observatory (DSCOVR). \n",
    "\n",
    "[DSCOVR](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ngdc.stp.swx:satellite-systems_dscovr) is equipped with a range of sensors designed to predict these storms by collecting vital data on the speed, temperature, and density of incoming solar plasma.\n",
    "\n",
    "[DSCOVR](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ngdc.stp.swx:satellite-systems_dscovr) maintains a orbit, stationed at Lagrange point one, positioned 1.5 million kilometers from Earth and situated between our planet and the Sun. NOAA uses this data to simulate Earth's magnetic field and atmosphere, offering potential early warnings of geomagnetic storms.\n",
    "\n",
    "\n",
    "Geomagnetic activity is commonly quantified using the $D_{st}$ index. Using solar wind parameters and magnetic field data, we can potentially predict changes in the $D_{st}$ index. Prior studies recommend incorporating interplanetary parameters as inputs [[1]](https://www.nature.com/articles/s41598-022-11721-8), including the interplanetary magnetic field ($IMF$), solar wind ($SW$), and in some cases, specific components like the IMF $B_z$, $SW$ electric field, temperature, speed, and density.\n",
    "\n",
    "It is worth noting that the study by Tasistro-Hart et al. incorporates observations from the solar disk as additional input data, suggesting that this inclusion enhances forecasting reliability. Furthermore, they shift their focus from predicting $D_{st}$ to the external component of geomagnetic storms, known as $E_{st}$, and their neural network architecture is adept at forecasting uncertainty.\n",
    "\n",
    "Note that,\n",
    "\n",
    "$D_{st} = I_{st}(t) + E_{st}(t)$\n",
    "\n",
    "In this notebook, we build upon prior research in the field, specifically the work outlined in [[2]](https://www.semanticscholar.org/paper/Probabilistic-Geomagnetic-Storm-Forecasting-via-Tasistro-Hart-Grayver/45103139959e86620de7353615db70e4e730efe5). \n",
    "\n",
    "Our approach involves implementing a machine learning model while accounting for the impact of hardware faults and anomalies. Finally, our goal is to provide uncertainty estimates for the output, following the methodology described by Tasistro-Hart et al.\n",
    "\n",
    "However, it's important to note that meticulous data preparation for training and validation significantly influence the performance of the machine learning model.\n",
    "\n",
    "### **Data Resources**\n",
    "\n",
    "For this project, we will directly utilize raw [DSCOVR](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ngdc.stp.swx:satellite-systems_dscovr) data as input. Given our objective of building a near-real-time system, we have opted to integrate the most recent data by implementing a function that can download datasets from the [Experimental Data Repository](https://www.spaceappschallenge.org/develop-the-oracle-of-dscovr-experimental-data-repository/).\n",
    "\n",
    "These datasets encompass data starting from 2016 and continue to receive updates to the present day. They will be stored in the `dataset` folder. Each `.csv` file contains 53 columns, with column 0 representing the time in UTC in the following format:  `YYYY-MM-DD hh:mm:ss`. Columns 1-3 correspond to the magnetic field components measured in nanoteslas (nT) at the time indicated in column 0. The remaining columns contain dimensional measurements from the Faraday cup plasma detector.\n",
    "\n",
    "#### **References**\n",
    "[1] [Prominence of the training data preparation in geomagnetic storm prediction using deep neural networks](https://www.nature.com/articles/s41598-022-11721-8)\n",
    "\n",
    "[2] [Probabilistic Geomagnetic Storm Forecasting via Deep Learning](https://www.semanticscholar.org/paper/Probabilistic-Geomagnetic-Storm-Forecasting-via-Tasistro-Hart-Grayver/45103139959e86620de7353615db70e4e730efe5)\n",
    "- [DSCOVR: Deep Space Climate Observatory](https://www.nesdis.noaa.gov/current-satellite-missions/currently-flying/dscovr-deep-space-climate-observatory)\n",
    "- [DSCOVR (Deep Space Climate Observatory) -eoPortal](https://www.eoportal.org/satellite-missions/dscovr)\n",
    "- [Deep Space Climate Observatory (DSCOVR)](https://www.nist.gov/measuring-cosmos/deep-space-climate-observatory-dscovr)\n",
    "\n",
    "\n",
    "## **Data Retrieval and Preprocessing**\n",
    "\n",
    "For this prototype, we utilise the experimental data provided during the competition. It is worth noting that in a real-time production environment, the functionality of the following function can be extended to facilitate the real-time acquisition and storage of data in a MongoDB database for efficient retrieval and processing.\n",
    "\n",
    "A virtual environment was created and the required libraries were installed to develop this project. Navigate to the project folder and type the following:\n",
    "\n",
    "```\n",
    "python -m venv ./venv\n",
    "\n",
    "pip install -r requirements.txt\n",
    "\n",
    "``` \n",
    "\n",
    "To activate the virtual environment please type:\n",
    "\n",
    "```\n",
    "source ./venv/bin/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries for data retrival and pro-processing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mtp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas: 2.1.1\n",
      "Numpy: 1.26.0\n",
      "matplotlib: 3.8.0\n"
     ]
    }
   ],
   "source": [
    "# check versions\n",
    "\n",
    "print(\"Pandas: {}\".format(pd.__version__))\n",
    "print(\"Numpy: {}\".format(np.__version__))\n",
    "print(\"matplotlib: {}\".format(mtp.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "root_url = \"https://opensource.gsfc.nasa.gov/spaceappschallenge/\"\n",
    "# The data start being recorded from 2016; DSCOVR became operational on July 27, 2016.\n",
    "start_year = 2016\n",
    "today = dt.date.today()\n",
    "current_year = today.year\n",
    "\n",
    "dataset_folder = \"../dataset\"  # Dataset forlder to store .csv files/unzipped data\n",
    "tmp_folder = \"../tmp\"  # Temporary folder to store downloaded .zip files\n",
    "os.makedirs(tmp_folder, exist_ok=True)\n",
    "os.makedirs(dataset_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "def fetch_experimental_dscovr_data():\n",
    "    \"\"\"Download and store experimental DSCOVR data.\n",
    "\n",
    "    The data is stored in .zip folders, which are first downloaded to a tmp folder,\n",
    "    and then extracted into the dataset folder before the zip files are erased.\n",
    "\n",
    "    Note: If the file is from the current year, it is always downloaded, even if it already exists.\n",
    "    \"\"\"\n",
    "    for year in range(start_year, current_year + 1):\n",
    "        url = root_url + \"dsc_fc_summed_spectra_{}_v01.zip\".format(year)\n",
    "        zip_filename = os.path.join(tmp_folder, \"dscovr_data_{}.zip\".format(year))\n",
    "        dataset_year_folder = os.path.join(dataset_folder, str(year))\n",
    "\n",
    "        # Remove existing files for the current year (if they exist)\n",
    "        if year == current_year:\n",
    "            existing_files = [\n",
    "                f\n",
    "                for f in os.listdir(dataset_year_folder)\n",
    "                if f.startswith(f\"dsc_fc_summed_spectra_{year}\")\n",
    "            ]\n",
    "            for existing_file in existing_files:\n",
    "                os.remove(os.path.join(dataset_year_folder, existing_file))\n",
    "\n",
    "        # Check if the file already exists, if not, download it\n",
    "        if not any(\n",
    "            f.startswith(f\"dsc_fc_summed_spectra_{year}\")\n",
    "            for f in os.listdir(dataset_year_folder)\n",
    "        ):\n",
    "            try:\n",
    "                # Download the zip file\n",
    "                print(\"Downloading DSCOVR data for year {}...\".format(year))\n",
    "                response = requests.get(url, stream=True)\n",
    "                total_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    # Display a progress bar when downloading experimental DSCOVR dataset\n",
    "                    with open(zip_filename, \"wb\") as file, tqdm(\n",
    "                        desc=zip_filename,\n",
    "                        total=total_size,\n",
    "                        unit=\"B\",\n",
    "                        unit_scale=True,\n",
    "                        unit_divisor=1024,\n",
    "                    ) as bar:\n",
    "                        for data in response.iter_content(chunk_size=1024):\n",
    "                            file.write(data)\n",
    "                            bar.update(len(data))\n",
    "                    print(\"Download complete for year {}.\".format(year))\n",
    "\n",
    "                    # Unzip the downloaded file into the dataset folder\n",
    "                    with zipfile.ZipFile(zip_filename, \"r\") as zip_ref:\n",
    "                        zip_ref.extractall(dataset_year_folder)\n",
    "\n",
    "                    # Delete the downloaded zip file from the temporary folder\n",
    "                    os.remove(zip_filename)\n",
    "                else:\n",
    "                    print(\"Failed to download data for year {}.\".format(year))\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    \"An error occurred while downloading DSCOVR data for year {}: {}\".format(\n",
    "                        year, e\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            print(\"DSCOVR Data for year {} already exists.\".format(year))\n",
    "\n",
    "\n",
    "def prepare_experimental_Kp_data():\n",
    "    dataset_folder = '../dataset/'\n",
    "    kp_folder = 'kp_data'\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def prapare_dataframe():\n",
    "    \"\"\"Loop through the .csv files to create a Pandas DataFrame.\n",
    "\n",
    "    This function reads CSV files from the dataset folder for the specified range of years\n",
    "    and combines them into a single Pandas DataFrame. It handles missing files gracefully.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A combined DataFrame containing data from all available CSV files.\n",
    "    \"\"\"\n",
    "\n",
    "    combined_df = pd.DataFrame()\n",
    "    dataset_folder = '../dataset/'\n",
    "    for year in range(start_year, current_year + 1):\n",
    "        path_csv_file = os.path.join(dataset_folder, str(year), f\"dsc_fc_summed_spectra_{year}_v01.csv\")\n",
    "        # Check if the file exists before trying to read it\n",
    "        if os.path.exists(path_csv_file):\n",
    "            df = pd.read_csv(\n",
    "                path_csv_file,\n",
    "                delimiter=\",\",\n",
    "                parse_dates=[0],\n",
    "                na_values=\"0\",\n",
    "                header=None,\n",
    "            )\n",
    "            combined_df = pd.concat([combined_df, df])\n",
    "        else:\n",
    "            print(f\"Warning: File {path_csv_file} does not exist.\")\n",
    "            \n",
    "    return combined_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the functions to download data and create a dataframe\n",
    "\n",
    "fetch_experimental_dscovr_data()\n",
    "\n",
    "df = prapare_dataframe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploratory Data Analysis (EDA)**\n",
    "\n",
    "In this section, we employ Pandas and data visualization libraries to gain a deeper understanding of the experimental DSCOVR data. Our aim is to identify patterns that will inform the selection of the most suitable machine learning model for our problem statement.\n",
    "\n",
    "We begin by providing a high-level overview of the data. Subsequently, we proceed with data cleaning, addressing issues such as missing values. Next, we delve into the exploration of correlations between variables, presenting our findings through visual representations.  Finally, we summarise the key findings from this section.\n",
    "\n",
    "### **Data Summary**\n",
    "\n",
    "Following is a high level overview of the data. Additional information on how to interpret the columns can be found [here](https://hpde.io/NASA/NumericalData/OMNI/PT1H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>6.83609</td>\n",
       "      <td>-3.37934</td>\n",
       "      <td>-12.9205</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-01 00:01:00</td>\n",
       "      <td>6.76732</td>\n",
       "      <td>-3.30194</td>\n",
       "      <td>-12.9967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-01 00:02:00</td>\n",
       "      <td>6.39107</td>\n",
       "      <td>-2.61173</td>\n",
       "      <td>-13.3271</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-01 00:03:00</td>\n",
       "      <td>6.44897</td>\n",
       "      <td>-2.61525</td>\n",
       "      <td>-13.3299</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-01 00:04:00</td>\n",
       "      <td>6.58758</td>\n",
       "      <td>-2.73082</td>\n",
       "      <td>-13.2361</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0        1        2        3   4   5   6   7   8   9   ...  \\\n",
       "0 2016-01-01 00:00:00  6.83609 -3.37934 -12.9205 NaN NaN NaN NaN NaN NaN  ...   \n",
       "1 2016-01-01 00:01:00  6.76732 -3.30194 -12.9967 NaN NaN NaN NaN NaN NaN  ...   \n",
       "2 2016-01-01 00:02:00  6.39107 -2.61173 -13.3271 NaN NaN NaN NaN NaN NaN  ...   \n",
       "3 2016-01-01 00:03:00  6.44897 -2.61525 -13.3299 NaN NaN NaN NaN NaN NaN  ...   \n",
       "4 2016-01-01 00:04:00  6.58758 -2.73082 -13.2361 NaN NaN NaN NaN NaN NaN  ...   \n",
       "\n",
       "   44  45  46  47  48  49  50  51  52  53  \n",
       "0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "1 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "2 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "3 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "4 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "\n",
    "# A note regarding column 4-53: 5th+ columns: These are the \"raw\" measurements of the spectrum from the plasma detector so they are the \"level 1\" data. \n",
    "# We're reading it as each value is the flow strength of the solar wind, with each column being an interval in a range of flow speeds at which the solar wind is traveling at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3277440 entries, 0 to 175679\n",
      "Data columns (total 54 columns):\n",
      " #   Column  Dtype         \n",
      "---  ------  -----         \n",
      " 0   0       datetime64[ns]\n",
      " 1   1       float64       \n",
      " 2   2       float64       \n",
      " 3   3       float64       \n",
      " 4   4       float64       \n",
      " 5   5       float64       \n",
      " 6   6       float64       \n",
      " 7   7       float64       \n",
      " 8   8       float64       \n",
      " 9   9       float64       \n",
      " 10  10      float64       \n",
      " 11  11      float64       \n",
      " 12  12      float64       \n",
      " 13  13      float64       \n",
      " 14  14      float64       \n",
      " 15  15      float64       \n",
      " 16  16      float64       \n",
      " 17  17      float64       \n",
      " 18  18      float64       \n",
      " 19  19      float64       \n",
      " 20  20      float64       \n",
      " 21  21      float64       \n",
      " 22  22      float64       \n",
      " 23  23      float64       \n",
      " 24  24      float64       \n",
      " 25  25      float64       \n",
      " 26  26      float64       \n",
      " 27  27      float64       \n",
      " 28  28      float64       \n",
      " 29  29      float64       \n",
      " 30  30      float64       \n",
      " 31  31      float64       \n",
      " 32  32      float64       \n",
      " 33  33      float64       \n",
      " 34  34      float64       \n",
      " 35  35      float64       \n",
      " 36  36      float64       \n",
      " 37  37      float64       \n",
      " 38  38      float64       \n",
      " 39  39      float64       \n",
      " 40  40      float64       \n",
      " 41  41      float64       \n",
      " 42  42      float64       \n",
      " 43  43      float64       \n",
      " 44  44      float64       \n",
      " 45  45      float64       \n",
      " 46  46      float64       \n",
      " 47  47      float64       \n",
      " 48  48      float64       \n",
      " 49  49      float64       \n",
      " 50  50      float64       \n",
      " 51  51      float64       \n",
      " 52  52      float64       \n",
      " 53  53      float64       \n",
      "dtypes: datetime64[ns](1), float64(53)\n",
      "memory usage: 1.3 GB\n"
     ]
    }
   ],
   "source": [
    "# providing a concise summary of a dataframe.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3277440"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return the number of rows\n",
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3277440</td>\n",
       "      <td>3.259998e+06</td>\n",
       "      <td>3.259998e+06</td>\n",
       "      <td>3.259998e+06</td>\n",
       "      <td>1.519804e+06</td>\n",
       "      <td>1.582311e+06</td>\n",
       "      <td>1.721032e+06</td>\n",
       "      <td>1.809991e+06</td>\n",
       "      <td>1.922099e+06</td>\n",
       "      <td>2.008476e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>179435.000000</td>\n",
       "      <td>131650.000000</td>\n",
       "      <td>103457.000000</td>\n",
       "      <td>70114.000000</td>\n",
       "      <td>61407.000000</td>\n",
       "      <td>37701.000000</td>\n",
       "      <td>34288.000000</td>\n",
       "      <td>29597.000000</td>\n",
       "      <td>28458.000000</td>\n",
       "      <td>26270.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2019-11-24 16:42:18.717038592</td>\n",
       "      <td>8.015442e-02</td>\n",
       "      <td>-1.255092e-01</td>\n",
       "      <td>2.171055e-02</td>\n",
       "      <td>6.904954e+01</td>\n",
       "      <td>2.405151e+01</td>\n",
       "      <td>9.262740e+01</td>\n",
       "      <td>9.369958e+01</td>\n",
       "      <td>1.246801e+02</td>\n",
       "      <td>1.251655e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>410.258331</td>\n",
       "      <td>353.559684</td>\n",
       "      <td>397.477883</td>\n",
       "      <td>381.584273</td>\n",
       "      <td>351.028206</td>\n",
       "      <td>364.846009</td>\n",
       "      <td>364.814574</td>\n",
       "      <td>339.140779</td>\n",
       "      <td>387.293616</td>\n",
       "      <td>339.150626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>-2.014280e+01</td>\n",
       "      <td>-3.178510e+01</td>\n",
       "      <td>-3.332440e+01</td>\n",
       "      <td>2.317260e-01</td>\n",
       "      <td>2.317260e-01</td>\n",
       "      <td>2.317260e-01</td>\n",
       "      <td>2.317260e-01</td>\n",
       "      <td>2.317260e-01</td>\n",
       "      <td>2.317260e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>96.621000</td>\n",
       "      <td>63.826300</td>\n",
       "      <td>63.398300</td>\n",
       "      <td>2.675280</td>\n",
       "      <td>2.947210</td>\n",
       "      <td>59.301300</td>\n",
       "      <td>76.164100</td>\n",
       "      <td>65.362400</td>\n",
       "      <td>0.231726</td>\n",
       "      <td>2.469710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2018-01-04 11:59:45</td>\n",
       "      <td>-2.532720e+00</td>\n",
       "      <td>-2.640140e+00</td>\n",
       "      <td>-1.522248e+00</td>\n",
       "      <td>3.139748e+01</td>\n",
       "      <td>2.317260e-01</td>\n",
       "      <td>4.230860e+01</td>\n",
       "      <td>3.983035e+01</td>\n",
       "      <td>5.333525e+01</td>\n",
       "      <td>4.449867e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>392.940000</td>\n",
       "      <td>336.209750</td>\n",
       "      <td>387.257000</td>\n",
       "      <td>372.588500</td>\n",
       "      <td>339.152500</td>\n",
       "      <td>365.221000</td>\n",
       "      <td>370.332750</td>\n",
       "      <td>341.495000</td>\n",
       "      <td>397.712750</td>\n",
       "      <td>343.118500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2020-03-20 23:59:30</td>\n",
       "      <td>1.694915e-01</td>\n",
       "      <td>-2.131885e-01</td>\n",
       "      <td>3.084400e-02</td>\n",
       "      <td>5.893845e+01</td>\n",
       "      <td>1.063320e+01</td>\n",
       "      <td>8.518255e+01</td>\n",
       "      <td>8.183820e+01</td>\n",
       "      <td>1.085690e+02</td>\n",
       "      <td>1.006845e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>421.542000</td>\n",
       "      <td>367.358000</td>\n",
       "      <td>416.448000</td>\n",
       "      <td>409.860500</td>\n",
       "      <td>380.525000</td>\n",
       "      <td>397.663000</td>\n",
       "      <td>397.949500</td>\n",
       "      <td>373.665000</td>\n",
       "      <td>431.640000</td>\n",
       "      <td>381.959500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2021-10-10 23:59:15</td>\n",
       "      <td>2.672278e+00</td>\n",
       "      <td>2.395460e+00</td>\n",
       "      <td>1.553040e+00</td>\n",
       "      <td>9.426612e+01</td>\n",
       "      <td>3.352490e+01</td>\n",
       "      <td>1.191900e+02</td>\n",
       "      <td>1.199030e+02</td>\n",
       "      <td>1.635685e+02</td>\n",
       "      <td>1.546990e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>437.436000</td>\n",
       "      <td>385.025750</td>\n",
       "      <td>432.293000</td>\n",
       "      <td>423.639000</td>\n",
       "      <td>397.720000</td>\n",
       "      <td>418.133000</td>\n",
       "      <td>419.557750</td>\n",
       "      <td>394.069000</td>\n",
       "      <td>451.927750</td>\n",
       "      <td>401.169250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2023-05-02 23:59:00</td>\n",
       "      <td>3.304940e+01</td>\n",
       "      <td>2.789380e+01</td>\n",
       "      <td>3.483770e+01</td>\n",
       "      <td>1.675760e+03</td>\n",
       "      <td>1.582720e+03</td>\n",
       "      <td>1.736050e+03</td>\n",
       "      <td>1.496590e+03</td>\n",
       "      <td>1.699290e+03</td>\n",
       "      <td>1.848460e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>1756.870000</td>\n",
       "      <td>1757.440000</td>\n",
       "      <td>1775.960000</td>\n",
       "      <td>1762.550000</td>\n",
       "      <td>1689.330000</td>\n",
       "      <td>1719.110000</td>\n",
       "      <td>1939.020000</td>\n",
       "      <td>1852.740000</td>\n",
       "      <td>1875.050000</td>\n",
       "      <td>1866.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.410268e+00</td>\n",
       "      <td>3.765791e+00</td>\n",
       "      <td>2.982616e+00</td>\n",
       "      <td>6.505515e+01</td>\n",
       "      <td>4.948382e+01</td>\n",
       "      <td>7.784311e+01</td>\n",
       "      <td>8.532521e+01</td>\n",
       "      <td>1.040021e+02</td>\n",
       "      <td>1.253738e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>66.313573</td>\n",
       "      <td>66.450856</td>\n",
       "      <td>72.205229</td>\n",
       "      <td>84.113211</td>\n",
       "      <td>85.454188</td>\n",
       "      <td>99.350766</td>\n",
       "      <td>104.976669</td>\n",
       "      <td>109.746891</td>\n",
       "      <td>118.891957</td>\n",
       "      <td>112.803118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  0             1             2   \\\n",
       "count                        3277440  3.259998e+06  3.259998e+06   \n",
       "mean   2019-11-24 16:42:18.717038592  8.015442e-02 -1.255092e-01   \n",
       "min              2016-01-01 00:00:00 -2.014280e+01 -3.178510e+01   \n",
       "25%              2018-01-04 11:59:45 -2.532720e+00 -2.640140e+00   \n",
       "50%              2020-03-20 23:59:30  1.694915e-01 -2.131885e-01   \n",
       "75%              2021-10-10 23:59:15  2.672278e+00  2.395460e+00   \n",
       "max              2023-05-02 23:59:00  3.304940e+01  2.789380e+01   \n",
       "std                              NaN  3.410268e+00  3.765791e+00   \n",
       "\n",
       "                 3             4             5             6             7   \\\n",
       "count  3.259998e+06  1.519804e+06  1.582311e+06  1.721032e+06  1.809991e+06   \n",
       "mean   2.171055e-02  6.904954e+01  2.405151e+01  9.262740e+01  9.369958e+01   \n",
       "min   -3.332440e+01  2.317260e-01  2.317260e-01  2.317260e-01  2.317260e-01   \n",
       "25%   -1.522248e+00  3.139748e+01  2.317260e-01  4.230860e+01  3.983035e+01   \n",
       "50%    3.084400e-02  5.893845e+01  1.063320e+01  8.518255e+01  8.183820e+01   \n",
       "75%    1.553040e+00  9.426612e+01  3.352490e+01  1.191900e+02  1.199030e+02   \n",
       "max    3.483770e+01  1.675760e+03  1.582720e+03  1.736050e+03  1.496590e+03   \n",
       "std    2.982616e+00  6.505515e+01  4.948382e+01  7.784311e+01  8.532521e+01   \n",
       "\n",
       "                 8             9   ...             44             45  \\\n",
       "count  1.922099e+06  2.008476e+06  ...  179435.000000  131650.000000   \n",
       "mean   1.246801e+02  1.251655e+02  ...     410.258331     353.559684   \n",
       "min    2.317260e-01  2.317260e-01  ...      96.621000      63.826300   \n",
       "25%    5.333525e+01  4.449867e+01  ...     392.940000     336.209750   \n",
       "50%    1.085690e+02  1.006845e+02  ...     421.542000     367.358000   \n",
       "75%    1.635685e+02  1.546990e+02  ...     437.436000     385.025750   \n",
       "max    1.699290e+03  1.848460e+03  ...    1756.870000    1757.440000   \n",
       "std    1.040021e+02  1.253738e+02  ...      66.313573      66.450856   \n",
       "\n",
       "                  46            47            48            49            50  \\\n",
       "count  103457.000000  70114.000000  61407.000000  37701.000000  34288.000000   \n",
       "mean      397.477883    381.584273    351.028206    364.846009    364.814574   \n",
       "min        63.398300      2.675280      2.947210     59.301300     76.164100   \n",
       "25%       387.257000    372.588500    339.152500    365.221000    370.332750   \n",
       "50%       416.448000    409.860500    380.525000    397.663000    397.949500   \n",
       "75%       432.293000    423.639000    397.720000    418.133000    419.557750   \n",
       "max      1775.960000   1762.550000   1689.330000   1719.110000   1939.020000   \n",
       "std        72.205229     84.113211     85.454188     99.350766    104.976669   \n",
       "\n",
       "                 51            52            53  \n",
       "count  29597.000000  28458.000000  26270.000000  \n",
       "mean     339.140779    387.293616    339.150626  \n",
       "min       65.362400      0.231726      2.469710  \n",
       "25%      341.495000    397.712750    343.118500  \n",
       "50%      373.665000    431.640000    381.959500  \n",
       "75%      394.069000    451.927750    401.169250  \n",
       "max     1852.740000   1875.050000   1866.960000  \n",
       "std      109.746891    118.891957    112.803118  \n",
       "\n",
       "[8 rows x 54 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic descriptive statistics for all columns\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53],\n",
       "      dtype='int64')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return the list of column names of the dataframe\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Cleaning**\n",
    "\n",
    "In this section, our focus is on data cleaning, which involves addressing missing values and outliers early in the process. Missing values can adversely affect the accuracy of a machine learning model. There are a couple of options for handling them: you can either drop rows or columns with missing values, or you can fill in the missing values with the median, mean, or mode. To determine the best approach, you can count the number of `NaN` values in the dataframe and also understand the significance of these missing valies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           0\n",
       "1       17442\n",
       "2       17442\n",
       "3       17442\n",
       "4     1757636\n",
       "5     1695129\n",
       "6     1556408\n",
       "7     1467449\n",
       "8     1355341\n",
       "9     1268964\n",
       "10    1148804\n",
       "11    1053137\n",
       "12     921593\n",
       "13     853886\n",
       "14     732934\n",
       "15     682399\n",
       "16     623358\n",
       "17     554684\n",
       "18     506961\n",
       "19     472456\n",
       "20     425829\n",
       "21     425497\n",
       "22     434438\n",
       "23     521356\n",
       "24     573915\n",
       "25     749402\n",
       "26     846987\n",
       "27    1001870\n",
       "28    1124509\n",
       "29    1315618\n",
       "30    1457869\n",
       "31    1690322\n",
       "32    1784844\n",
       "33    2032300\n",
       "34    2114747\n",
       "35    2218250\n",
       "36    2396398\n",
       "37    2499624\n",
       "38    2570894\n",
       "39    2726544\n",
       "40    2775096\n",
       "41    2936463\n",
       "42    2979187\n",
       "43    3035621\n",
       "44    3098005\n",
       "45    3145790\n",
       "46    3173983\n",
       "47    3207326\n",
       "48    3216033\n",
       "49    3239739\n",
       "50    3243152\n",
       "51    3247843\n",
       "52    3248982\n",
       "53    3251170\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of missing values\n",
    "df.isnull().sum()\n",
    "# we have 3277440 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a paper by Smith, A. W., Forsyth, C., Rae, I. J., Garton, T. M., Jackman, C. M., Bakrania, M., et al. (2022), it is noted that despite DSCOVR data being collected in real-time, there are instances where data from ACE are used to fill in missing data for larger intervals. Given the short duration of this project (2 days), we have chosen to employ limited interpolation to address missing values for gaps of 5 minutes or less, as suggested in the paper.\n",
    "\n",
    "Due to time constraints, we will use this method. There are different types of interpolation that can be applied to time series data, including:\n",
    "- linear interpolation (very common)\n",
    "- Polynomial interpolatin\n",
    "- Spline interpolation\n",
    "- Time-Based Interpolation\n",
    "- Exponential Smoothing\n",
    "- Seasonal Decomposition\n",
    "\n",
    "For this project we used linear interpolation for columns 4-53."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_interpolation(df:  pd.DataFrame, init_column: int, end_column: int, gap: int):\n",
    "    \"\"\"\n",
    "    Linearly interpolate missing values in specified columns of a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the data to be interpolated.\n",
    "    init_column (int): The index of the first column to start interpolation.\n",
    "    end_column (int): The index of the last column to end interpolation (exclusive).\n",
    "    gap (int): The maximum gap size (in minutes) to be filled with interpolation.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with missing values interpolated.\n",
    "    \"\"\"\n",
    "    # Select the columns to be interpolated\n",
    "    columns_to_interpolate = df.columns[init_column:end_column]\n",
    "    # Apply linear interpolation to the specified columns\n",
    "    df[columns_to_interpolate] = df[columns_to_interpolate].interpolate(method='linear', limit=gap)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Engineering** \n",
    "\n",
    "We also undertake feature engineering to create new attributes necessary for training our machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
